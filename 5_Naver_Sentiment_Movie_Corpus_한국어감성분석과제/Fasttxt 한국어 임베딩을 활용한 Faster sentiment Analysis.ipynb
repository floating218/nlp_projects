{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttxt 한국어 임베딩을 활용한 Faster sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 모델 요약\n",
    "|항목|내용|\n",
    "|-|-|\n",
    "|한국어 형태소 분석기|Mecab|\n",
    "|한국어 워드 임베딩|fasttext|\n",
    "|사용 모델|Faster sentiment analysis|\n",
    "|Loss|BCELoss|\n",
    "|Optimizer|Adam|\n",
    "|learning rate|0.001|\n",
    "\n",
    "---\n",
    "\n",
    "## 결과 요약\n",
    "\n",
    "|항목|데이터개수|accuracy(%)|\n",
    "|-|-|-|\n",
    "|train set|132286|87.8|\n",
    "|validation set|14699|84.5|\n",
    "|test set|43295|85.4|\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 한국어 텍스트 전처리 \n",
    "    - 한국어 형태소 분석기: Mecab를 사용\n",
    "    \n",
    "\n",
    "2. 한국어 워드 임베딩 Load\n",
    "    - https://github.com/Kyubyong/wordvectors에서 pretrained 한국어 word vector를 가져와서 임베딩\n",
    "    \n",
    "\n",
    "3. 학습용, 테스트용 데이터셋 준비하기\n",
    "\n",
    "\n",
    "4. 모델 build 및 학습\n",
    "    - CNN 모델을 이용하여 학습\n",
    "    \n",
    "    \n",
    "5. 영화리뷰 샘플을 가지고 predict해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 한국어 텍스트 전처리 및 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Mecab,Okt,Komoran,Hannanum,Kkma\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어를 처리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 아 더빙 .. 진짜 짜증나 네요 목소리'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_korean(sentence):\n",
    "    \n",
    "    sentence=sentence.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    \n",
    "    #형태소로 분해하는 코드\n",
    "    tokenizer=Kkma()\n",
    "    sentence=tokenizer.morphs(sentence)\n",
    "\n",
    "    #stopword는 제외\n",
    "    stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    sentence=[word for word in sentence if not word in stopwords]\n",
    "    \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "process_korean('아 더빙.. 진짜 짜증나네요 목소리')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_korean_embedding(train,label): #영화리뷰 셋을 한국어 처리를 해주는 코드\n",
    "    train_lst=[]\n",
    "    label_lst=[]\n",
    "    i=0\n",
    "    for sentence in train:\n",
    "        try:\n",
    "            train_lst.append(process_korean(sentence))\n",
    "            label_lst.append(label[i])\n",
    "        except:\n",
    "            pass\n",
    "        i+=1\n",
    "    return train_lst,label_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_table('data/ratings_train.txt')\n",
    "X_train,Y_train=to_korean_embedding(list(train_data['document']),list(train_data['label']))\n",
    "len(X_train),len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.read_table('data/ratings_test.txt')\n",
    "X_test,Y_test=to_korean_embedding(list(test_data['document']),list(test_data['label']))\n",
    "len(X_test),len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# f=open('x_train_komoran.pickle','wb')\n",
    "# pickle.dump(X_train,f)\n",
    "# f=open('y_train_komoran.pickle','wb')\n",
    "# pickle.dump(Y_train,f)\n",
    "# f=open('x_test_komoran.pickle','wb')\n",
    "# pickle.dump(X_test,f)\n",
    "# f=open('y_test_komoran.pickle','wb')\n",
    "# pickle.dump(Y_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장해둔 가공 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('data/x_train_mecab.pickle','rb')\n",
    "X_train=pickle.load(f)\n",
    "f=open('data/y_train_mecab.pickle','rb')\n",
    "Y_train=pickle.load(f)\n",
    "f=open('data/x_test_mecab.pickle','rb')\n",
    "X_test=pickle.load(f)\n",
    "f=open('data/y_test_mecab.pickle','rb')\n",
    "Y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary를 만들고 단어를 정수 index로 변환하기 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    # maximum 문장 길이만큼 padding해줌\n",
    "    \n",
    "    features = np.full((len(reviews_ints), seq_length), '<pad>') #일단 모두 pad로 채우고\n",
    "    \n",
    "    for i, row in enumerate(reviews_ints): #원본 데이터에 대해 \n",
    "        features[i, :len(row)] = np.array(row)[:seq_length+1]  #[문장 , 원본데이터의 길이 까지] = np.array(원본데이터) [  :95번째까지]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def a_text_to_idx(wordlst,vocab_to_int):\n",
    "    return [vocab_to_int[word] for word in wordlst]\n",
    "    \n",
    "\n",
    "\n",
    "def text_to_embedding(X_train,Y_train,vocab_to_int):\n",
    "    reviews_split=X_train #['아 더빙 .. 진짜 짜증나다 목소리',...]\n",
    "    encoded_labels=Y_train\n",
    "\n",
    "    reviews_split=[r.split(\" \") for r in reviews_split] #[['아', '더빙', '..', '진짜', '짜증나다', '목소리']]\n",
    "\n",
    "    #단어 수가 0인 것 제외\n",
    "    try:\n",
    "        non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review) != 0]\n",
    "        reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "        encoded_labels = [encoded_labels[ii] for ii in non_zero_idx]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #정해진 길이의 embedding에 <pad>를 추가하기\n",
    "    seq_length=max(list(map(len,reviews_split)))\n",
    "    reviews_padded=pad_features(reviews_split,seq_length=seq_length) #[['아', '더빙', '..', '진짜', '짜증나다', '목소리', 'pad','pad'...]]\n",
    "    \n",
    "    reviews_ints = []\n",
    "    \n",
    "    #vocabulary 사전에 따라 숫자로 변환하기\n",
    "    for review in reviews_padded:\n",
    "        sublst=[]\n",
    "        for word in review:\n",
    "            try:\n",
    "                sublst.append(vocab_to_int[word])\n",
    "            except:\n",
    "                pass\n",
    "        reviews_ints.append(sublst) #[[숫자로 변환됨]]\n",
    "\n",
    "    # word 개수가 seq length가 되는 것만 남김\n",
    "    try:\n",
    "        non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) == seq_length]\n",
    "        reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "        encoded_labels = [encoded_labels[ii] for ii in non_zero_idx]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return np.array(reviews_ints,dtype=int),np.array(encoded_labels,dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 워드 임베딩 Load\n",
    "### Pretrained embedding을 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('data/ko/ko.vec')\n",
    "\n",
    "# 단어 리스트 작성\n",
    "vocab = model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어벡터 추출\n",
    "wordvectors = []\n",
    "for v in vocab:\n",
    "    wordvectors.append(model.wv[v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 데이터+pretrained embedding으로 vocabulary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(reviews_split):\n",
    "    #vocabulary 사전을 제작함 \n",
    "    \n",
    "    all_text = ' '.join(reviews_split)\n",
    "    words = all_text.split()\n",
    "    counts = Counter(words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii+1 for ii, word in enumerate(vocab, 1)}\n",
    "    vocab_to_int['<pad>']=1\n",
    "    return vocab_to_int\n",
    "\n",
    "def make_pretrained_embedding(vocab_to_int,vocab_model, embedding_model):\n",
    "    totallst=list(set(vocab_to_int.keys()) or set(vocab_model))\n",
    "    pretrained_embedding=[]\n",
    "    new_vocab_to_int=dict()\n",
    "    idx=0\n",
    "    ukn=0\n",
    "    for aword in totallst:\n",
    "        if aword in vocab_model:\n",
    "            pretrained_embedding.append(embedding_model[vocab_model.index(aword)])\n",
    "        elif aword == \"<pad>\":\n",
    "            pretrained_embedding.append(np.zeros(200))\n",
    "        else:\n",
    "            pretrained_embedding.append(np.random.normal(scale=0.6, size=(200, )))\n",
    "            ukn+=1\n",
    "        new_vocab_to_int[aword]=idx\n",
    "        idx+=1\n",
    "    print(ukn)\n",
    "    return new_vocab_to_int,pretrained_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38240\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int=make_vocab(X_train)\n",
    "new_vocab_to_int, pretrained_embedding=make_pretrained_embedding(vocab_to_int,vocab,wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54027, 54027)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_vocab_to_int.keys())),len(list(vocab_to_int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54027, 54027)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totallst=list(set(vocab_to_int.keys()) or set(vocab))\n",
    "len(totallst),len(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습용, 테스트용 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train셋 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,encoded_labels=text_to_embedding(X_train,Y_train,new_vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test셋 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=text_to_embedding(X_test,Y_test,new_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((146985, 116), (146985,), (43295, 105), (43295,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape,encoded_labels.shape,test_x.shape,test_y.shape\n",
    "# len(features),len(encoded_labels),len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set, validation set을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.9\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x,train_y=features,encoded_labels\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t132286 \n",
      "Validation set: \t14699 \n",
      "Test set: \t\t43295\n"
     ]
    }
   ],
   "source": [
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
    "       \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
    "      \"\\nTest set: \\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "seq_lengths=iter(train_loader).next()[0].size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 116])\n",
      "Sample input: \n",
      " tensor([[30695, 29037, 21966,  ...,   364,   364,   364],\n",
      "        [47654, 45965, 48667,  ...,   364,   364,   364],\n",
      "        [36825, 32567, 22835,  ...,   364,   364,   364],\n",
      "        ...,\n",
      "        [53968, 14840, 18908,  ...,   364,   364,   364],\n",
      "        [49381, 46923, 27370,  ...,   364,   364,   364],\n",
      "        [  836, 35935, 40478,  ...,   364,   364,   364]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "train_on_gpu=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "This model has far fewer parameters than the previous model as it only has 2 layers that have any parameters, the embedding layer and the linear layer. There is no RNN component in sight!\n",
    "\n",
    "Instead, it first calculates the word embedding for each word using the `Embedding` layer (blue), then calculates the average of all of the word embeddings (pink) and feeds this through the `Linear` layer (silver), and that's it!\n",
    "\n",
    "![](assets/sentiment8.png)\n",
    "\n",
    "We implement the averaging with the `avg_pool2d` (average pool 2-dimensions) function. Initially, you may think using a 2-dimensional pooling seems strange, surely our sentences are 1-dimensional, not 2-dimensional? However, you can think of the word embeddings as a 2-dimensional grid, where the words are along one axis and the dimensions of the word embeddings are along the other. The image below is an example sentence after being converted into 5-dimensional word embeddings, with the words along the vertical axis and the embeddings along the horizontal axis. Each element in this [4x5] tensor is represented by a green block.\n",
    "\n",
    "![](assets/sentiment9.png)\n",
    "\n",
    "The `avg_pool2d` uses a filter of size `embedded.shape[1]` (i.e. the length of the sentence) by 1. This is shown in pink in the image below.\n",
    "\n",
    "![](assets/sentiment10.png)\n",
    "\n",
    "We calculate the average value of all elements covered by the filter, then the filter then slides to the right, calculating the average over the next column of embedding values for each word in the sentence. \n",
    "\n",
    "![](assets/sentiment11.png)\n",
    "\n",
    "Each filter position gives us a single value, the average of all covered elements. After the filter has covered all embedding dimensions we get a [1x5] tensor. This tensor is then passed through the linear layer to produce our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "        self.sig=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text.T)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "#         print(pooled)\n",
    "        output=self.fc(pooled)\n",
    "#         print(output)\n",
    "        return self.sig(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding=torch.tensor(pretrained_embedding)\n",
    "\n",
    "# INPUT_DIM = len(vocab_to_int)+1\n",
    "INPUT_DIM =pretrained_embedding.shape[0]\n",
    "# EMBEDDING_DIM = seq_lengths\n",
    "EMBEDDING_DIM = 200\n",
    "OUTPUT_DIM = 1\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "net = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    net.zero_grad()\n",
    "\n",
    "    # get the output from the model\n",
    "    output = net(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "\n",
    "    \n",
    "    ooo=output.squeeze()\n",
    "    lll=labels.float()\n",
    "\n",
    "    loss = criterion(output.squeeze(), labels.float())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding=torch.tensor(pretrained_embedding)\n",
    "\n",
    "# INPUT_DIM = len(vocab_to_int)+1\n",
    "INPUT_DIM =pretrained_embedding.shape[0]\n",
    "# EMBEDDING_DIM = seq_lengths\n",
    "EMBEDDING_DIM = 200\n",
    "OUTPUT_DIM = 1\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "net = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,805,601 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(net):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델에 임베딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54027, 200])\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6295,  0.6276,  1.3237,  ..., -0.9714, -0.6111,  0.4917],\n",
       "        [ 0.0548, -0.1073,  0.3068,  ...,  0.0617, -0.0241, -0.7517],\n",
       "        [ 1.0211, -1.3973, -0.1416,  ..., -0.6790, -0.4101, -0.2291],\n",
       "        ...,\n",
       "        [-0.0580, -0.1129, -0.4677,  ..., -0.2866,  0.0102,  0.5035],\n",
       "        [ 0.1461, -0.0324, -0.0583,  ..., -0.1438, -0.0773,  0.2297],\n",
       "        [-0.2916,  1.4892,  0.3274,  ...,  1.0402,  0.3334,  0.0900]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training\n",
    "\n",
    "Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. You can also add code to save a model by name.\n",
    "\n",
    ">We'll also be using a new kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "We also have some data and training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# training params\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "counter = 0\n",
    "print_every = 500\n",
    "clip=5 # gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 500... Loss: 0.533874...\n",
      "Epoch: 1/4... Step: 1000... Loss: 0.389470...\n",
      "Epoch: 1/4... Step: 1500... Loss: 0.429910...\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.424488...\n",
      "Epoch: 1/4... Step: 2500... Loss: 0.407559...\n",
      "Train accuracy: 0.796\n",
      "Valid accuracy: 0.842\n",
      "\n",
      "Epoch: 2/4... Step: 3000... Loss: 0.454814...\n",
      "Epoch: 2/4... Step: 3500... Loss: 0.248076...\n",
      "Epoch: 2/4... Step: 4000... Loss: 0.648734...\n",
      "Epoch: 2/4... Step: 4500... Loss: 0.371138...\n",
      "Epoch: 2/4... Step: 5000... Loss: 0.400335...\n",
      "Train accuracy: 0.856\n",
      "Valid accuracy: 0.848\n",
      "\n",
      "Epoch: 3/4... Step: 5500... Loss: 0.326036...\n",
      "Epoch: 3/4... Step: 6000... Loss: 0.306989...\n",
      "Epoch: 3/4... Step: 6500... Loss: 0.273832...\n",
      "Epoch: 3/4... Step: 7000... Loss: 0.293503...\n",
      "Epoch: 3/4... Step: 7500... Loss: 0.348679...\n",
      "Train accuracy: 0.869\n",
      "Valid accuracy: 0.851\n",
      "\n",
      "Epoch: 4/4... Step: 8000... Loss: 0.367335...\n",
      "Epoch: 4/4... Step: 8500... Loss: 0.341367...\n",
      "Epoch: 4/4... Step: 9000... Loss: 0.324770...\n",
      "Epoch: 4/4... Step: 9500... Loss: 0.418877...\n",
      "Epoch: 4/4... Step: 10000... Loss: 0.222321...\n",
      "Epoch: 4/4... Step: 10500... Loss: 0.241432...\n",
      "Train accuracy: 0.878\n",
      "Valid accuracy: 0.845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# move model to GPU, if available\n",
    "if(train_on_gpu):    \n",
    "    net.cuda()    \n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    num_correct = 0\n",
    "    \n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "           \n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output = net(inputs)\n",
    "        # calculate the loss and perform backprop\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # compare predictions to true label\n",
    "        pred = torch.round(output.squeeze()) \n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)    \n",
    "        \n",
    "    \n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            \n",
    "            val_num_correct=0\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output = net(inputs)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred = torch.round(output.squeeze()) \n",
    "                correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "                correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "                val_num_correct += np.sum(correct)           \n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 )    \n",
    "    # -- stats! -- ##\n",
    "    train_acc = num_correct/len(train_loader.dataset)\n",
    "    print(\"Train accuracy: {:.3f}\".format(train_acc))\n",
    "    valid_acc = val_num_correct/len(valid_loader.dataset)\n",
    "    print(\"Valid accuracy: {:.3f}\".format(valid_acc))   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"cnn_model.pickle\",'wb')\n",
    "pickle.dump(net,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.359\n",
      "Test accuracy: 0.854\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output= net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 영화리뷰 샘플을 가지고 predict해보기\n",
    "\n",
    "### Try out test_reviews of your own!\n",
    "\n",
    "You can change this test_review to any text that you want. Read it and think: is it pos or neg? Then see if your model predicts correctly!\n",
    "    \n",
    "> **Exercise:** Write a `predict` function that takes in a trained net, a plain text_review, and a sequence length, and prints out a custom statement for a positive or negative review!\n",
    "* You can use any functions that you've already defined or define any helper functions you want to complete `predict`, but it should just take in a trained net, a text review, and a sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_text_to_idx(wordlst,vocab_to_int):\n",
    "    return [vocab_to_int[word] for word in wordlst]\n",
    "\n",
    "def predict(net, test_review, label, new_vocab_to_int, sequence_length=95):\n",
    "    try:    \n",
    "        net.eval()\n",
    "        test_review=process_korean(test_review)\n",
    "        test_review=test_review.split(\" \")\n",
    "        seq_length=sequence_length\n",
    "        padded_review=[\"<pad>\"]*seq_length\n",
    "        for i in range(len(test_review)):\n",
    "            padded_review[i]=test_review[i]\n",
    "        idx_review=a_text_to_idx(padded_review,new_vocab_to_int)\n",
    "        idx_review=[idx_review]\n",
    "        features=np.array(idx_review)\n",
    "        feature_tensor = torch.from_numpy(features)\n",
    "        batch_size = feature_tensor.size(0)\n",
    "        if(train_on_gpu):\n",
    "            feature_tensor = feature_tensor.cuda()\n",
    "        # get the output from the model\n",
    "        output = net(feature_tensor)\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())            \n",
    "        print('예측값 : {:.6f}'.format(output.item()))\n",
    "        if(pred.item()==1):\n",
    "            print(\"예측 : positive 리뷰입니다.\")\n",
    "        else:\n",
    "            print(\"예측 : negative 리뷰입니다.\")\n",
    "    except:    \n",
    "        print(\"없는 단어 입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.read_table('ratings_test.txt')\n",
    "test_label=test_data['label']\n",
    "test_data=test_data['document']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰들\n",
      "\n",
      "리뷰 : 기대 이상이었음\n",
      "예측값 : 0.470436\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : 역시 믿고 보는 감독이었다\n",
      "예측값 : 0.539401\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "리뷰 : 진짜 감동이었어요 ㅠㅠㅠㅠㅠㅠ \n",
      "예측값 : 0.759288\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "리뷰 : 역대급 띵작임\n",
      "예측값 : 0.412996\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "부정적인 리뷰들\n",
      "\n",
      "리뷰 : 이 영화 되게 노잼이다.\n",
      "예측값 : 0.303075\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : 배우가 발연기를 하네 ㅉㅉ\n",
      "예측값 : 0.102352\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : 돈이 아까웠음 진심...\n",
      "예측값 : 0.110114\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : ㄹㅇ 제작비 낭비 ㅠㅠㅠㅠ\n",
      "예측값 : 0.212369\n",
      "예측 : negative 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos=[\"기대 이상이었음\",\n",
    "    \"역시 믿고 보는 감독이었다\",\n",
    "    \"진짜 감동이었어요 ㅠㅠㅠㅠㅠㅠ \",\n",
    "    \"역대급 띵작임\"]\n",
    "\n",
    "neg=[\"이 영화 되게 노잼이다.\",\n",
    "    \"배우가 발연기를 하네 ㅉㅉ\",\n",
    "    \"돈이 아까웠음 진심...\",\n",
    "    \"ㄹㅇ 제작비 낭비 ㅠㅠㅠㅠ\"]\n",
    "\n",
    "print(\"긍정적인 리뷰들\\n\")\n",
    "\n",
    "for p in pos:\n",
    "    print(\"리뷰 : \"+p)\n",
    "    predict(net,p,1,new_vocab_to_int,200)\n",
    "    print()\n",
    "\n",
    "print(\"부정적인 리뷰들\\n\")\n",
    "    \n",
    "for n in neg:\n",
    "    print(\"리뷰 : \"+n)\n",
    "    predict(net,n,0,new_vocab_to_int,200)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 링크 \n",
    "https://github.com/DonghyungKo/NLP_sentiment_classification/blob/master/RNN/RNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
