{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttxt 한국어 임베딩을 활용한 Convolutional sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 모델 요약\n",
    "|항목|내용|\n",
    "|-|-|\n",
    "|한국어 형태소 분석기|Mecab|\n",
    "|한국어 워드 임베딩|fasttext|\n",
    "|사용 모델|CNN|\n",
    "|output 채널 개수|100|\n",
    "|filter 사이즈|3,4,5|\n",
    "|dropout 비율|0.5|\n",
    "|Loss|BCELoss|\n",
    "|Optimizer|Adam|\n",
    "|learning rate|0.001|\n",
    "\n",
    "---\n",
    "\n",
    "## 결과 요약\n",
    "\n",
    "|항목|데이터개수|accuracy(%)|\n",
    "|-|-|-|\n",
    "|train set|132286|96.9|\n",
    "|validation set|14699|85.5|\n",
    "|test set|43295|85.7|\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 한국어 텍스트 전처리 \n",
    "    - 한국어 형태소 분석기: Mecab를 사용\n",
    "    \n",
    "\n",
    "2. 한국어 워드 임베딩 Load\n",
    "    - https://github.com/Kyubyong/wordvectors에서 pretrained 한국어 word vector를 가져와서 임베딩\n",
    "    \n",
    "\n",
    "3. 학습용, 테스트용 데이터셋 준비하기\n",
    "\n",
    "\n",
    "4. 모델 build 및 학습\n",
    "    - CNN 모델을 이용하여 학습\n",
    "    \n",
    "    \n",
    "5. 영화리뷰 샘플을 가지고 predict해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 한국어 텍스트 전처리 및 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Mecab,Okt,Komoran,Hannanum,Kkma\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어를 처리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 아 더빙 .. 진짜 짜증나 네요 목소리'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_korean(sentence):\n",
    "    \n",
    "    sentence=sentence.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    \n",
    "    #형태소로 분해하는 코드\n",
    "    tokenizer=Kkma()\n",
    "    sentence=tokenizer.morphs(sentence)\n",
    "\n",
    "    #stopword는 제외\n",
    "    stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    sentence=[word for word in sentence if not word in stopwords]\n",
    "    \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "process_korean('아 더빙.. 진짜 짜증나네요 목소리')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_korean_embedding(train,label): #영화리뷰 셋을 한국어 처리를 해주는 코드\n",
    "    train_lst=[]\n",
    "    label_lst=[]\n",
    "    i=0\n",
    "    for sentence in train:\n",
    "        try:\n",
    "            train_lst.append(process_korean(sentence))\n",
    "            label_lst.append(label[i])\n",
    "        except:\n",
    "            pass\n",
    "        i+=1\n",
    "    return train_lst,label_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_table('ratings_train.txt')\n",
    "X_train,Y_train=to_korean_embedding(list(train_data['document']),list(train_data['label']))\n",
    "len(X_train),len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.read_table('ratings_test.txt')\n",
    "X_test,Y_test=to_korean_embedding(list(test_data['document']),list(test_data['label']))\n",
    "len(X_test),len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# f=open('x_train_komoran.pickle','wb')\n",
    "# pickle.dump(X_train,f)\n",
    "# f=open('y_train_komoran.pickle','wb')\n",
    "# pickle.dump(Y_train,f)\n",
    "# f=open('x_test_komoran.pickle','wb')\n",
    "# pickle.dump(X_test,f)\n",
    "# f=open('y_test_komoran.pickle','wb')\n",
    "# pickle.dump(Y_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장해둔 가공 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('x_train_mecab.pickle','rb')\n",
    "X_train=pickle.load(f)\n",
    "f=open('y_train_mecab.pickle','rb')\n",
    "Y_train=pickle.load(f)\n",
    "f=open('x_test_mecab.pickle','rb')\n",
    "X_test=pickle.load(f)\n",
    "f=open('y_test_mecab.pickle','rb')\n",
    "Y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary를 만들고 단어를 정수 index로 변환하기 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    # maximum 문장 길이만큼 padding해줌\n",
    "    \n",
    "    features = np.full((len(reviews_ints), seq_length), '<pad>') #일단 모두 pad로 채우고\n",
    "    \n",
    "    for i, row in enumerate(reviews_ints): #원본 데이터에 대해 \n",
    "        features[i, :len(row)] = np.array(row)[:seq_length+1]  #[문장 , 원본데이터의 길이 까지] = np.array(원본데이터) [  :95번째까지]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def a_text_to_idx(wordlst,vocab_to_int):\n",
    "    return [vocab_to_int[word] for word in wordlst]\n",
    "    \n",
    "\n",
    "\n",
    "def text_to_embedding(X_train,Y_train,vocab_to_int):\n",
    "    reviews_split=X_train #['아 더빙 .. 진짜 짜증나다 목소리',...]\n",
    "    encoded_labels=Y_train\n",
    "\n",
    "    reviews_split=[r.split(\" \") for r in reviews_split] #[['아', '더빙', '..', '진짜', '짜증나다', '목소리']]\n",
    "\n",
    "    #단어 수가 0인 것 제외\n",
    "    try:\n",
    "        non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review) != 0]\n",
    "        reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "        encoded_labels = [encoded_labels[ii] for ii in non_zero_idx]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #정해진 길이의 embedding에 <pad>를 추가하기\n",
    "    seq_length=max(list(map(len,reviews_split)))\n",
    "    reviews_padded=pad_features(reviews_split,seq_length=seq_length) #[['아', '더빙', '..', '진짜', '짜증나다', '목소리', 'pad','pad'...]]\n",
    "    \n",
    "    reviews_ints = []\n",
    "    \n",
    "    #vocabulary 사전에 따라 숫자로 변환하기\n",
    "    for review in reviews_padded:\n",
    "        sublst=[]\n",
    "        for word in review:\n",
    "            try:\n",
    "                sublst.append(vocab_to_int[word])\n",
    "            except:\n",
    "                pass\n",
    "        reviews_ints.append(sublst) #[[숫자로 변환됨]]\n",
    "\n",
    "    # word 개수가 seq length가 되는 것만 남김\n",
    "    try:\n",
    "        non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) == seq_length]\n",
    "        reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "        encoded_labels = [encoded_labels[ii] for ii in non_zero_idx]\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return np.array(reviews_ints,dtype=int),np.array(encoded_labels,dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 워드 임베딩 Load\n",
    "### Pretrained embedding을 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('data/ko/ko.vec')\n",
    "\n",
    "# 단어 리스트 작성\n",
    "vocab = model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어벡터 추출\n",
    "wordvectors = []\n",
    "for v in vocab:\n",
    "    wordvectors.append(model.wv[v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 데이터+pretrained embedding으로 vocabulary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(reviews_split):\n",
    "    #vocabulary 사전을 제작함 \n",
    "    \n",
    "    all_text = ' '.join(reviews_split)\n",
    "    words = all_text.split()\n",
    "    counts = Counter(words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii+1 for ii, word in enumerate(vocab, 1)}\n",
    "    vocab_to_int['<pad>']=1\n",
    "    return vocab_to_int\n",
    "\n",
    "def make_pretrained_embedding(vocab_to_int,vocab_model, embedding_model):\n",
    "    totallst=list(set(vocab_to_int.keys()) or set(vocab_model))\n",
    "    pretrained_embedding=[]\n",
    "    new_vocab_to_int=dict()\n",
    "    idx=0\n",
    "    ukn=0\n",
    "    for aword in totallst:\n",
    "        if aword in vocab_model:\n",
    "            pretrained_embedding.append(embedding_model[vocab_model.index(aword)])\n",
    "        elif aword == \"<pad>\":\n",
    "            pretrained_embedding.append(np.zeros(200))\n",
    "        else:\n",
    "            pretrained_embedding.append(np.random.normal(scale=0.6, size=(200, )))\n",
    "            ukn+=1\n",
    "        new_vocab_to_int[aword]=idx\n",
    "        idx+=1\n",
    "    print(ukn)\n",
    "    return new_vocab_to_int,pretrained_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38240\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int=make_vocab(X_train)\n",
    "new_vocab_to_int, pretrained_embedding=make_pretrained_embedding(vocab_to_int,vocab,wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54027, 54027)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_vocab_to_int.keys())),len(list(vocab_to_int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54027, 54027)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totallst=list(set(vocab_to_int.keys()) or set(vocab))\n",
    "len(totallst),len(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습용, 테스트용 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train셋 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,encoded_labels=text_to_embedding(X_train,Y_train,new_vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test셋 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=text_to_embedding(X_test,Y_test,new_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((146985, 116), (146985,), (43295, 105), (43295,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape,encoded_labels.shape,test_x.shape,test_y.shape\n",
    "# len(features),len(encoded_labels),len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set, validation set을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.9\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x,train_y=features,encoded_labels\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t132286 \n",
      "Validation set: \t14699 \n",
      "Test set: \t\t43295\n"
     ]
    }
   ],
   "source": [
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
    "       \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
    "      \"\\nTest set: \\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "seq_lengths=iter(train_loader).next()[0].size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 116])\n",
      "Sample input: \n",
      " tensor([[14195, 26006,    12,  ..., 28080, 28080, 28080],\n",
      "        [20698,   883, 22908,  ..., 28080, 28080, 28080],\n",
      "        [11228, 33540, 20847,  ..., 28080, 28080, 28080],\n",
      "        ...,\n",
      "        [ 5850, 47611, 46978,  ..., 28080, 28080, 28080],\n",
      "        [15077, 38553,  6026,  ..., 28080, 28080, 28080],\n",
      "        [49823, 47172,  2682,  ..., 28080, 28080, 28080]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 build 및 학습\n",
    "\n",
    "Now to build our model.\n",
    "\n",
    "The first major hurdle is visualizing how CNNs are used for text. Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. However, we know that the first step in almost all of our previous tutorials (and pretty much all NLP pipelines) is converting the words into word embeddings. This is how we can visualize our words in 2 dimensions, each word along one axis and the elements of vectors aross the other dimension. Consider the 2 dimensional representation of the embedded sentence below:\n",
    "\n",
    "![](assets/sentiment9.png)\n",
    "\n",
    "We can then use a filter that is **[n x emb_dim]**. This will cover $n$ sequential words entirely, as their width will be `emb_dim` dimensions. Consider the image below, with our word vectors are represented in green. Here we have 4 words with 5 dimensional embeddings, creating a [4x5] \"image\" tensor. A filter that covers two words at a time (i.e. bi-grams) will be **[2x5]** filter, shown in yellow, and each element of the filter with have a _weight_ associated with it. The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
    "\n",
    "![](assets/sentiment12.png)\n",
    "\n",
    "The filter then moves \"down\" the image (or across the sentence) to cover the next bi-gram and another output (weighted sum) is calculated. \n",
    "\n",
    "![](assets/sentiment13.png)\n",
    "\n",
    "Finally, the filter moves down again and the final output for this filter is calculated.\n",
    "\n",
    "![](assets/sentiment14.png)\n",
    "\n",
    "In our case (and in the general case where the width of the filter equals the width of the \"image\"), our output will be a vector with number of elements equal to the height of the image (or lenth of the word) minus the height of the filter plus one, $4-2+1=3$ in this case.\n",
    "\n",
    "This example showed how to calculate the output of one filter. Our model (and pretty much all CNNs) will have lots of these filters. The idea is that each filter will learn a different feature to extract. In the above example, we are hoping each of the **[2 x emb_dim]** filters will be looking for the occurence of different bi-grams. \n",
    "\n",
    "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for analysing sentiment of movie reviews.\n",
    "\n",
    "The next step in our model is to use *pooling* (specifically *max pooling*) on the output of the convolutional layers. This is similar to the FastText model where we performed the average over each of the word vectors, implemented by the `F.avg_pool2d` function, however instead of taking the average over a dimension, we are taking the maximum value over a dimension. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown is the activation function applied to the output of the convolutions).\n",
    "\n",
    "![](assets/sentiment15.png)\n",
    "\n",
    "The idea here is that the maximum value is the \"most important\" feature for determining the sentiment of the review, which corresponds to the \"most important\" n-gram within the review. How do we know what the \"most important\" n-gram is? Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that are highly indicative of the sentiment are seen, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output. \n",
    "\n",
    "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 n-grams and making a final decision. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implement the convolutional layers with `nn.Conv2d`. The `in_channels` argument is the number of \"channels\" in your image going into the convolutional layer. In actual images this is usually 3 (one channel for each of the red, blue and green channels), however when using text we only have a single channel, the text itself. The `out_channels` is the number of filters and the `kernel_size` is the size of the filters. Each of our `kernel_size`s is going to be **[n x emb_dim]** where $n$ is the size of the n-grams.\n",
    "\n",
    "In PyTorch, RNNs want the input with the batch dimension second, whereas CNNs want the batch dimension first - we do not have to permute the data here as we have already set `batch_first = True` in our `TEXT` field. We then pass the sentence through an embedding layer to get our embeddings. The second dimension of the input into a `nn.Conv2d` layer must be the channel dimension. As text technically does not have a channel dimension, we `unsqueeze` our tensor to create one. This matches with our `in_channels=1` in the initialization of our convolutional layers. \n",
    "\n",
    "We then pass the tensors through the convolutional and pooling layers, using the `ReLU` activation function after the convolutional layers. Another nice feature of the pooling layers is that they handle sentences of different lengths. The size of the output of the convolutional layer is dependent on the size of the input to it, and different batches contain sentences of different lengths. Without the max pooling layer the input to our linear layer would depend on the size of the input sentence (not what we want). One option to rectify this would be to trim/pad all sentences to the same length, however with the max pooling layer we always know the input to the linear layer will be the total number of filters. **Note**: there an exception to this if your sentence(s) are shorter than the largest filter used. You will then have to pad your sentences to the length of the largest filter. In the IMDb data there are no reviews only 5 words long so we don't have to worry about that, but you will if you are using your own data.\n",
    "\n",
    "Finally, we perform dropout on the concatenated filter outputs and then pass them through a linear layer to make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sig=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        output=self.fc(cat).squeeze(1)\n",
    "        \n",
    "        output=self.sig(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding=torch.tensor(pretrained_embedding)\n",
    "# INPUT_DIM = len(vocab_to_int)+1\n",
    "INPUT_DIM =pretrained_embedding.shape[0]\n",
    "# EMBEDDING_DIM = seq_lengths\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX=new_vocab_to_int['<pad>']\n",
    "\n",
    "net = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28080"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,046,001 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(net):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델에 임베딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54027, 200])\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3933,  0.1533,  0.2433,  ...,  0.0058,  0.4796, -0.8924],\n",
       "        [ 0.3508, -0.1666,  0.1610,  ...,  0.0368,  0.7583, -0.1848],\n",
       "        [-0.0462,  0.3320,  0.0487,  ..., -0.4848,  0.4125, -0.1326],\n",
       "        ...,\n",
       "        [ 0.0743, -0.0316,  0.2388,  ..., -0.1870, -0.0817, -0.1621],\n",
       "        [-0.2906, -0.1595,  0.3406,  ..., -0.1620, -0.2360, -0.4483],\n",
       "        [ 1.2834, -0.4482, -0.5768,  ..., -0.9720,  0.3555,  1.2703]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training\n",
    "\n",
    "Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. You can also add code to save a model by name.\n",
    "\n",
    ">We'll also be using a new kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "We also have some data and training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# training params\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "counter = 0\n",
    "print_every = 500\n",
    "clip=5 # gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 500... Loss: 0.426230...\n",
      "Epoch: 1/4... Step: 1000... Loss: 0.327731...\n",
      "Epoch: 1/4... Step: 1500... Loss: 0.380746...\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.377675...\n",
      "Epoch: 1/4... Step: 2500... Loss: 0.279272...\n",
      "Train accuracy: 0.829\n",
      "Valid accuracy: 0.855\n",
      "\n",
      "Epoch: 2/4... Step: 3000... Loss: 0.329996...\n",
      "Epoch: 2/4... Step: 3500... Loss: 0.280918...\n",
      "Epoch: 2/4... Step: 4000... Loss: 0.238523...\n",
      "Epoch: 2/4... Step: 4500... Loss: 0.237686...\n",
      "Epoch: 2/4... Step: 5000... Loss: 0.330080...\n",
      "Train accuracy: 0.895\n",
      "Valid accuracy: 0.865\n",
      "\n",
      "Epoch: 3/4... Step: 5500... Loss: 0.085578...\n",
      "Epoch: 3/4... Step: 6000... Loss: 0.157445...\n",
      "Epoch: 3/4... Step: 6500... Loss: 0.040780...\n",
      "Epoch: 3/4... Step: 7000... Loss: 0.173706...\n",
      "Epoch: 3/4... Step: 7500... Loss: 0.164687...\n",
      "Train accuracy: 0.938\n",
      "Valid accuracy: 0.858\n",
      "\n",
      "Epoch: 4/4... Step: 8000... Loss: 0.072164...\n",
      "Epoch: 4/4... Step: 8500... Loss: 0.151353...\n",
      "Epoch: 4/4... Step: 9000... Loss: 0.153644...\n",
      "Epoch: 4/4... Step: 9500... Loss: 0.116668...\n",
      "Epoch: 4/4... Step: 10000... Loss: 0.042935...\n",
      "Epoch: 4/4... Step: 10500... Loss: 0.188821...\n",
      "Train accuracy: 0.969\n",
      "Valid accuracy: 0.855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# move model to GPU, if available\n",
    "if(train_on_gpu):    \n",
    "    net.cuda()    \n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    num_correct = 0\n",
    "    \n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "           \n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output = net(inputs)\n",
    "        # calculate the loss and perform backprop\n",
    "\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # compare predictions to true label\n",
    "        pred = torch.round(output.squeeze()) \n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)    \n",
    "        \n",
    "    \n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            \n",
    "            val_num_correct=0\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output = net(inputs)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred = torch.round(output.squeeze()) \n",
    "                correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "                correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "                val_num_correct += np.sum(correct)           \n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 )    \n",
    "    # -- stats! -- ##\n",
    "    train_acc = num_correct/len(train_loader.dataset)\n",
    "    print(\"Train accuracy: {:.3f}\".format(train_acc))\n",
    "    valid_acc = val_num_correct/len(valid_loader.dataset)\n",
    "    print(\"Valid accuracy: {:.3f}\".format(valid_acc))   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"cnn_model.pickle\",'wb')\n",
    "pickle.dump(net,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.452\n",
      "Test accuracy: 0.857\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output= net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 영화리뷰 샘플을 가지고 predict해보기\n",
    "\n",
    "### Try out test_reviews of your own!\n",
    "\n",
    "You can change this test_review to any text that you want. Read it and think: is it pos or neg? Then see if your model predicts correctly!\n",
    "    \n",
    "> **Exercise:** Write a `predict` function that takes in a trained net, a plain text_review, and a sequence length, and prints out a custom statement for a positive or negative review!\n",
    "* You can use any functions that you've already defined or define any helper functions you want to complete `predict`, but it should just take in a trained net, a text review, and a sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_text_to_idx(wordlst,vocab_to_int):\n",
    "    return [vocab_to_int[word] for word in wordlst]\n",
    "\n",
    "def predict(net, test_review, label, new_vocab_to_int, sequence_length=95):\n",
    "    try:    \n",
    "        net.eval()\n",
    "        test_review=process_korean(test_review)\n",
    "        test_review=test_review.split(\" \")\n",
    "        seq_length=sequence_length\n",
    "        padded_review=[\"<pad>\"]*seq_length\n",
    "        for i in range(len(test_review)):\n",
    "            padded_review[i]=test_review[i]\n",
    "        idx_review=a_text_to_idx(padded_review,new_vocab_to_int)\n",
    "        idx_review=[idx_review]\n",
    "        features=np.array(idx_review)\n",
    "        feature_tensor = torch.from_numpy(features)\n",
    "        batch_size = feature_tensor.size(0)\n",
    "        if(train_on_gpu):\n",
    "            feature_tensor = feature_tensor.cuda()\n",
    "        # get the output from the model\n",
    "        output = net(feature_tensor)\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())            \n",
    "        print('예측값 : {:.6f}'.format(output.item()))\n",
    "        if(pred.item()==1):\n",
    "            print(\"예측 : positive 리뷰입니다.\")\n",
    "        else:\n",
    "            print(\"예측 : negative 리뷰입니다.\")\n",
    "    except:    \n",
    "        print(\"없는 단어 입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.read_table('ratings_test.txt')\n",
    "test_label=test_data['label']\n",
    "test_data=test_data['document']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰들\n",
      "\n",
      "리뷰 : 기대 이상이었음\n",
      "예측값 : 0.520270\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "리뷰 : 역시 믿고 보는 감독이었다\n",
      "예측값 : 0.989716\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "리뷰 : 진짜 감동이었어요 ㅠㅠㅠㅠㅠㅠ \n",
      "예측값 : 0.998165\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "리뷰 : 역대급 띵작임\n",
      "예측값 : 0.715748\n",
      "예측 : positive 리뷰입니다.\n",
      "\n",
      "부정적인 리뷰들\n",
      "\n",
      "리뷰 : 이 영화 되게 노잼이다.\n",
      "예측값 : 0.001697\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : 배우가 발연기를 하네 ㅉㅉ\n",
      "예측값 : 0.227050\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : 돈이 아까웠음 진심...\n",
      "예측값 : 0.000986\n",
      "예측 : negative 리뷰입니다.\n",
      "\n",
      "리뷰 : ㄹㅇ 제작비 낭비 ㅠㅠㅠㅠ\n",
      "예측값 : 0.006756\n",
      "예측 : negative 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos=[\"기대 이상이었음\",\n",
    "    \"역시 믿고 보는 감독이었다\",\n",
    "    \"진짜 감동이었어요 ㅠㅠㅠㅠㅠㅠ \",\n",
    "    \"역대급 띵작임\"]\n",
    "\n",
    "neg=[\"이 영화 되게 노잼이다.\",\n",
    "    \"배우가 발연기를 하네 ㅉㅉ\",\n",
    "    \"돈이 아까웠음 진심...\",\n",
    "    \"ㄹㅇ 제작비 낭비 ㅠㅠㅠㅠ\"]\n",
    "\n",
    "print(\"긍정적인 리뷰들\\n\")\n",
    "\n",
    "for p in pos:\n",
    "    print(\"리뷰 : \"+p)\n",
    "    predict(net,p,1,new_vocab_to_int,200)\n",
    "    print()\n",
    "\n",
    "print(\"부정적인 리뷰들\\n\")\n",
    "    \n",
    "for n in neg:\n",
    "    print(\"리뷰 : \"+n)\n",
    "    predict(net,n,0,new_vocab_to_int,200)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 링크 \n",
    "https://github.com/DonghyungKo/NLP_sentiment_classification/blob/master/RNN/RNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
