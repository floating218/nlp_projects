{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "본 노트북에서 당신은 pytorch 사용법을 소개받을 것입니다. pytorch란 신경망을 만들고 트레이닝하는 프레임웍입니다. pytorch는 다양한 방식으로 numpy에서 사용하는 array같이 행동할 수 있죠. 이 numpy array는 tensor라고 합니다. pytorch는 이 tensor를 가지고 GPU로 옮겨서 빠른 프로세싱을 가능하게 합니다. 또한 pytorch는 gradient(for backpropagation)을 자동으로 계산해주는 module를 제공하고 신경망을 만들기 위한 다른 특별한 모듈을 제공합니다. pytorch는 python과 numpy, scipy stack과 보다 더 coherent합니다 다른 프레임웍에 비해서는 말이죠.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "딥러닝은 인공 신경망에 기반합니다. 인공 신경망은 1950년대 말부터 그 개념이 존재했죠. 이 망은 뉴런과 유사한 부분들로부터 만들어지는데요, 이 뉴론은 '뉴론'이나 'unit'이라고 부릅니다. 이 unit은 여러개의 weighted된 input을 가집니다. 이 weighted input은 다 함께 합쳐지고 (선형 결합을 통해서 합쳐지죠) 그리고 나서 활성화 함수를 통과한다음 결국에는 unit의 output을 얻게 됩니다. \n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "수학적으로는 이런 형태를 띱니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "벡터의 내적(dot,innter product라고 부릅니다)\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "신경망의 계산은 \"tensor\"에 대한 선형대수 operation의 bunch입니다. tensor란 매트릭스들의 generalization입니다. 벡터는 1차원의 tensor라고 할 수 있죠. 매트릭스는 2차원의 tensor이고, 3개의 index를 가진 array는 3차원의 tensor죠. (예를들어 RGB 색상 이미지). 신경망의 기본적인 데이터 구조는 tensor이고 (다른 딥러닝 프레임워크도 비슷합니다만) tensor를 중심으로 pytorch가 만들어졌습니다.  \n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "기본적인건 다 봤으니, 어떻게 pytorch를 쓰고 간단한 신경망을 만드는 방법을 알아봅시다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#활성화 함수 = 시그모이드 함수 = 1/1+e^x\n",
    "\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) #  random seed를 7로 지정하였습니다. \n",
    "\n",
    "# Features are 3 random normal variables\n",
    "# torch.randn : ranodm normal variable을 생성해준다. \n",
    "# torch.randon ((행의차원,열의차원))\n",
    "features = torch.randn((1, 5))\n",
    "\n",
    "# torch.randn_like(다른 텐서) : 다른 텐서와 같은 모양의 텐서를 똑같이 만든다.\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))\n",
    "\n",
    "features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상단에 나는 간단한 네트워크의 output을 얻기 위해 쓸 수 있는 data를 만들었어요. 이것은 지금은 랜덤하게 만들어졌지만, 우리는 normal data를 사용할 것입니다. \n",
    "\n",
    "`features = torch.randn((1, 5))` : (1,5) 모양의 tensor를 만듭니다. 1행, 5열. normal distribution을 통해 랜덤으로 만들어져 평균은 0, 표준편차는 1입니다. \n",
    "\n",
    "`weights = torch.randn_like(features)`: feature와 똑같은 모양의 tensor를 만들어줍니다. 역시 정규분포입니다. \n",
    "\n",
    "Finally, `bias = torch.randn((1, 1))` : 정규분포로 1개의 값을 만들어줍니다. \n",
    "\n",
    "pytorch tensor는 덧셈, 곱셈, 뺄셈 등등이 모두 가능합니다. numpy처럼요. 보통은, 당신은 pytorch tensor를 numpy를 사용하는 것과 같은 방법으로 사용할 것입니다. 그것들은 큰 장점이죠. 지금은 이렇게 만들어진 데이터를 가지고 output을 계산할 겁니다. \n",
    "\n",
    "> **Exercise**:  input ( `features`, `weights`, `bias`)을 가지고 네트워크의 output을 계산하세요. 넘파이 처럼 pytorch도  [`torch.sum()`] 함수를 쓸수 있습니다. (https://pytorch.org/docs/stable/torch.html#torch.sum). `.sum()` method도 사용 가능하죠. 위에서 정의함 `activation`을 가지고 활성화 함수로 사용하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution\n",
    "\n",
    "# Now, make our labels from our data and true weights\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "y = activation((features * weights).sum() + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "너는 곱셈도 할 수 있고 덧셈도 할 수 있다. 보통 너는 matrix 곱셈을 하고 싶을 것이다. 왜냐하면 매트릭스 곱셈은 GPU를 통한 high-performance 계산과 최신의 library를 사용하여 할 수 있는 가장 효율적인 것이기 때문이다. \n",
    "\n",
    "여기, 우리는 feature와 weight에 대한 matrix 곱셈을 하고 싶으면, [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm)를 쓴다.  또는  [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul)을 쓴다. 근데 이걸 features나 weight을 가지고 하려고 하면 에러가 날 것이다.\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "네가 프레임워크를 통해 신경망을 만들때마다 이런 상황을 종종 보게될 것이다. 이게 무슨 일이냐면 우리의 tensor가 맞는 모양이 아니라서 곱셈을 못한다는 뜻이다. 행렬 곱셈을 하려면 첫번째 텐서의 행의 수가 두번째 텐서의 열의 수가 같아야 한다. 그런데 feature와 weight의 모양이 똑같이 `(1, 5)`이다. 두번째 텐서인 weights의 모양을 바꿔줘야 한다. \n",
    "\n",
    "**Note:** tensor의 모양을 보고 싶으면 `tensor.shape`를 사용하세요. \n",
    "\n",
    "텐서의 모양을 바꾸는 여러개의 옵션이 있다. [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` : `(a,b)` 모양의 텐서로 어떻게든 만들어서 return한다. 새로운 텐서를 메모리의 다른 부분에 저장한다.\n",
    "\n",
    "* `weights.resize_(a, b)` : `(a,b)` 모양의 텐서로 만들어서 return한다. 특이점은, 만약 원래 텐서보다 더 적은 크기의 모양을 요구할 경우 텐서의 일부 데이터는 날려버린다 (메모리에는 그대로 있음). 만약 원래 텐서보다 더 큰 크기의 모양을 요구할 경우 새로운 원소는 메모리에서ㅓ uninitialize된다. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "`.view()`를 사용하면 위 3가지 방법 중 아무거나 사용하게 된다. something like `weights.view(5, 1)`.\n",
    "\n",
    "> **Exercise**: Calculate the output of our little network using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8948],\n",
       "         [-0.3556],\n",
       "         [ 1.2324],\n",
       "         [ 0.1382],\n",
       "         [-1.6822]]), tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Solution\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "weights.view(5,1),weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack them up!\n",
    "\n",
    "하나의 뉴런을 가지고 output을 계산하는 방법이다. 이 알고리즘의 힘은 이 unit들을 레이어와 레이어의 스택으로 쌓을 때 나타납니다. 이 레이어의 output은 다음 layer의 input이 됩니다. 여러개의 input unit과 output unit을 가지고 matrix로서 weight을 표현할 필요가 있어요. \n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "첫번째 레이어는 바닥에 나타나있는데, **input layer**라고 부릅니다. 중간의 레이어는 **hidden layer**라고 부릅니다. 마지막 레이어는 **output layer**입니다. 우리는 이 네트워크를 수학적으로 표현할  수 있으며 행렬의 곱셈을 통해 선형 결합을 시킵니다. 한 operation이 각 unit에 있습니다. 예를 들어 hidden layer에서 ($h_1$ and $h_2$ here) 계산될 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이 작은 네트워크의 output은 히든 레이어를 output unit에 대한 input으로 취급하면서 형성됩니다. 이 네트워크 output는 간단히\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-0032437f2024>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-0032437f2024>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1개의 layer: h1, h2\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1개의 layer: h1, h2\n",
    "h1:x1,x2,x3을 w11,w21,w31과 곱한 벡터\n",
    "h2:x1,x2,x3을 w12,w22,w32과 곱한 벡터\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 코드\n",
    "- torch.manual_seed(n)\n",
    "- torch.randn((행,렬))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "\n",
    "#우선 랜덤 상태를 설정할 수 있습니다. \n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# features: 1*3 크기의 벡터로, 랜덤으로 설정된 숫자들\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "\n",
    "#n_input: features의 차원수\n",
    "#n_hidden: hidden unit의 개수(h1,h2)\n",
    "#n_output: 최종 결과 unit의 개수(1)\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "# W1: h1,h2를 만들기 위한 w를 가진 매트릭스 (feature차원수, h1h2개수 2개)\n",
    "# feature차원수 X (feature차원수 , h1,h2개수 2개) --> (결과값 h1,h2 2개)\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "\n",
    "# Weights for hidden layer to output layer\n",
    "# (h1,h2개수 2개) X (h1,h2개수 2개 , output개수) --> (output개수)\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([1, 2]) torch.Size([2, 1]) torch.Size([1, 1])\n",
      "torch.Size([1, 3]) torch.Size([1, 2]) torch.Size([1, 1]) tensor([[0.3171]])\n",
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "### Solution\n",
    "\n",
    "#사이즈정리\n",
    "#input=(1,3)\n",
    "#W1=(3,2)\n",
    "#B1=(1,2)\n",
    "#H=(1,2)\n",
    "#W2=(2,1)\n",
    "#B2=(1,1)\n",
    "#output=(1,1)\n",
    "\n",
    "#input = (1,3)\n",
    "\n",
    "#input -> (layer1) -> h (1,2)\n",
    "h = activation(torch.mm(features, W1) + B1)\n",
    "\n",
    "# h -> (layer2) -> output (1,1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "\n",
    "print(W1.shape,B1.shape,W2.shape,B2.shape)\n",
    "print(features.shape,h.shape,output.shape,output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 당신이 맞게했다면, output이 `tensor([[0.3171]])`로 나오는 것을 확인할 수 있었을 것이다. \n",
    "\n",
    "hidden unit의 개수는 **hyperparameter**라고 불린다. 이는 weights와 bias와는 구분된다. 우리가 뉴럴넷을 학습시키면서 다시 보겠지만, 더 많은 hidden unit이 있을수록, 더 많은 layer가 있는것이며, data로부터 더많이 learn해서 정확한 prediction을 가능하게 해준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "보너스 섹션이다. pytorch는 numpy array를 torch tensor로 변환해주는 기능을 가지고 있다. tensor를 numpy로부터 생성하기 위해서 `torch.from_numpy()`를 사용한다. 반대로 tensor를 numpy array로 바꾸려면 `.numpy()`만 붙여주면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57011002, 0.90625321, 0.14391065],\n",
       "       [0.67422288, 0.29745701, 0.74718076],\n",
       "       [0.29785311, 0.44560056, 0.42835224],\n",
       "       [0.91017425, 0.47195771, 0.80450331]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5701, 0.9063, 0.1439],\n",
       "        [0.6742, 0.2975, 0.7472],\n",
       "        [0.2979, 0.4456, 0.4284],\n",
       "        [0.9102, 0.4720, 0.8045]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57011002, 0.90625321, 0.14391065],\n",
       "       [0.67422288, 0.29745701, 0.74718076],\n",
       "       [0.29785311, 0.44560056, 0.42835224],\n",
       "       [0.91017425, 0.47195771, 0.80450331]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1402, 1.8125, 0.2878],\n",
       "        [1.3484, 0.5949, 1.4944],\n",
       "        [0.5957, 0.8912, 0.8567],\n",
       "        [1.8203, 0.9439, 1.6090]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14022005, 1.81250642, 0.2878213 ],\n",
       "       [1.34844576, 0.59491401, 1.49436152],\n",
       "       [0.59570622, 0.89120113, 0.85670449],\n",
       "       [1.82034849, 0.94391543, 1.60900662]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
