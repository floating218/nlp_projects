{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/pic7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "****\n",
    "## Step 1: Sentence segmentation\n",
    "\n",
    "전체 텍스트 (문단들 전체)를 문장으로 나누는 것이다. 보통 온점을 기준으로 문장을 나눈다. 그러나  문서가 깨끗하지 않으면 다른 복잡한 기술이 필요하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step2: Word Tokenization\n",
    "\n",
    "문장을 얻었으니, 단어word 단위로 문장을 쪼갤 수 있다. 이를 tokenization이라고 하고 쪼개진 단위는 word 또는 token이라고도 한다. 공백이 있는 영어 같은 언어에서 쉬운 편이다. 온점을 token의 분리 단위에 포함시키기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step3: Predicting Parts of Speech for Each Token\n",
    "\n",
    "<img src=\"data/pic1.png\">\n",
    "\n",
    "<img src=\"data/pic2.png\">\n",
    "\n",
    "token이 noun, verb, adjective 등인지 구분하는 것이 필요하다. 각 word의 역할을 아는것은 문장이 뭘 말하고 있는지 아는데 도움을 준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step4: Text Lemmatization\n",
    "\n",
    "<img src=\"data/pic3.png\">\n",
    "\n",
    "I had a pony.\n",
    "\n",
    "I had two ponies\n",
    "\n",
    "두 문장 모두 noun pony에 대해 이야기하고 있으나 inflection이 다르다. 두 단어를 다른 단어로 인식하는 것은 곤란하다. lemmatization은 단어의 lemma(basic form)을 알아내는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step5: Stop Words\n",
    "\n",
    "<img src=\"data/pic4.png\">\n",
    "\n",
    "stopword를 filter out하고 통계 분석을 해야 한다. 왜냐하면 너무 자주 나와서 노이즈를 발생시키기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step6: Dependancy Parsing\n",
    "\n",
    "<img src=\"data/pic5.png\">\n",
    "\n",
    "\n",
    "모든 단어가 자신의 문장에서 서로 어떻게 연관되는지를 알아내는 것을 지칭한다. \n",
    "목적은 하나의 parent word에서 뻗어나간 트리를 각 단어와 연결시키는 것이다. \n",
    "거기서 더 나아가, 두 단어 사이의 관계의 타입을 예측할 수도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step 6b: Finding Noun Phrases\n",
    "\n",
    "여태까지는 모든 단어를 각각 독립된 entity로 취급해왔다. 그러나 가끔 그룹핑해서 하나의 생각을 나타내는 것으로 간주하는 것이 더 말이 된다. 이걸해야하는지 아닌지는 우리의 목표에 달렸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step 7: Named Entity Recognition (NER)\n",
    "\n",
    "<img src=\"data/pic6.png\">\n",
    "\n",
    "NER의 목적은 단어가 나타내는 real-world 콘셉트에 따라 단어를 detect하고 레이블링하는 것이다. 예를 들어\n",
    "\n",
    "- 사람 이름\n",
    "- 회사 이름\n",
    "- 지리 위치\n",
    "- 물건 이름\n",
    "- 날짜 시간\n",
    "- 돈의 액수\n",
    "- 특별한 사건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Step 8: Conference Resolution\n",
    "\n",
    "<img src=\"data/pic6.png\">\n",
    "\n",
    "여기까지는 문장 하나를 파악했다. 그런데 영어에는 대명사가 있다. conference resolution은 대명사가 지칭하는 것을 파악하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London (GPE)\n",
      "England (GPE)\n",
      "the United Kingdom (GPE)\n",
      "the River Thames (LOC)\n",
      "Great Britain (GPE)\n",
      "London (GPE)\n",
      "two millennia (DATE)\n",
      "Romans (NORP)\n",
      "Londinium (LOC)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)\n",
    "\n",
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1950, [REDACTED] published his famous article \"Computing Machinery and Intelligence\". In 1957, [REDACTED] \n",
      "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Replace a token with \"REDACTED\" if it is a name\n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    else:\n",
    "        return token.string\n",
    "\n",
    "# Loop through all the entities in a document and check if they are names\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\"\n",
    "\n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London:\n",
      " - the capital and most populous city of England and  the United Kingdom.  \n",
      "\n",
      " - a major settlement  for two millennia.  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the document with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract semi-structured statements\n",
    "statements = textacy.extract.semistructured_statements(doc, \"London\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Here are the things I know about London:\")\n",
    "\n",
    "for statement in statements:\n",
    "    subject, verb, fact = statement\n",
    "    print(f\" - {fact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
