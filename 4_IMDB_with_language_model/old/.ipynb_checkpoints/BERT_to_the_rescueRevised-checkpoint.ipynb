{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert to the rescue!\n",
    "- based on https://towardsdatascience.com/bert-to-the-rescue-17671379687f\n",
    "- but changed imdb dataset (not from pytorch-nlp, but from a file, imdb_master.csv)\n",
    "- So preprossing is different from the original post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EnVIV6Vt8f4d",
    "outputId": "3c8e3d88-13b0-4652-973d-3fa2c023e48e"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUYrv06z8gaF"
   },
   "outputs": [],
   "source": [
    "random.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgkbhHcB17GY"
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ty24UrRjqIsb"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "26trq3gIrJeG",
    "outputId": "196b2aa3-0176-4010-baf8-e650ca0da658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'my', 'name', 'is', 'dim', '##a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Hi my name is Dima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_master.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.drop(['file'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review label\n",
       "0  test  Once again Mr. Costner has dragged out a movie...   neg\n",
       "1  test  This is an example of why the majority of acti...   neg\n",
       "2  test  First of all I hate those moronic rappers, who...   neg\n",
       "3  test  Not even the Beatles could write songs everyon...   neg\n",
       "4  test  Brass pictures (movies is not a fitting word f...   neg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['type'] == 'train'][:500].append(df[df['type'] == 'train'][-500:])\n",
    "test_df = df[df['type'] =='test'][:50].append(df[df['type'] =='test'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['review'].tolist()\n",
    "test_texts = test_df['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['label']\n",
    "test_labels = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 100\n",
      "1000 100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))\n",
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1k9rcOzQr5Zm",
    "outputId": "b9b59a4e-57bd-4e9c-fcd1-2a4ba1fadc0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
    "\n",
    "len(train_tokens), len(test_tokens)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'story',\n",
       " 'of',\n",
       " 'a',\n",
       " 'man',\n",
       " 'who',\n",
       " 'has',\n",
       " 'unnatural',\n",
       " 'feelings',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pig',\n",
       " '.',\n",
       " 'starts',\n",
       " 'out',\n",
       " 'with',\n",
       " 'a',\n",
       " 'opening',\n",
       " 'scene',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'terrific',\n",
       " 'example',\n",
       " 'of',\n",
       " 'absurd',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'a',\n",
       " 'formal',\n",
       " 'orchestra',\n",
       " 'audience',\n",
       " 'is',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'an',\n",
       " 'insane',\n",
       " ',',\n",
       " 'violent',\n",
       " 'mob',\n",
       " 'by',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'chanting',\n",
       " '##s',\n",
       " 'of',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'singers',\n",
       " '.',\n",
       " 'unfortunately',\n",
       " 'it',\n",
       " 'stays',\n",
       " 'absurd',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'time',\n",
       " 'with',\n",
       " 'no',\n",
       " 'general',\n",
       " 'narrative',\n",
       " 'eventually',\n",
       " 'making',\n",
       " 'it',\n",
       " 'just',\n",
       " 'too',\n",
       " 'off',\n",
       " 'putting',\n",
       " '.',\n",
       " 'even',\n",
       " 'those',\n",
       " 'from',\n",
       " 'the',\n",
       " 'era',\n",
       " 'should',\n",
       " 'be',\n",
       " 'turned',\n",
       " 'off',\n",
       " '.',\n",
       " 'the',\n",
       " 'cryptic',\n",
       " 'dialogue',\n",
       " 'would',\n",
       " 'make',\n",
       " 'shakespeare',\n",
       " 'seem',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'a',\n",
       " 'third',\n",
       " 'grade',\n",
       " '##r',\n",
       " '.',\n",
       " 'on',\n",
       " 'a',\n",
       " 'technical',\n",
       " 'level',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'better',\n",
       " 'than',\n",
       " 'you',\n",
       " 'might',\n",
       " 'think',\n",
       " 'with',\n",
       " 'some',\n",
       " 'good',\n",
       " 'cinematography',\n",
       " 'by',\n",
       " 'future',\n",
       " 'great',\n",
       " 'vi',\n",
       " '##lm',\n",
       " '##os',\n",
       " 'z',\n",
       " '##si',\n",
       " '##gm',\n",
       " '##ond',\n",
       " '.',\n",
       " 'future',\n",
       " 'stars',\n",
       " 'sally',\n",
       " 'kirk',\n",
       " '##land',\n",
       " 'and',\n",
       " 'frederic',\n",
       " 'forrest',\n",
       " 'can',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'briefly',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Ca7KKnhuT5c",
    "outputId": "69ba3b87-4d86-448b-c9cb-7be8361182f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 512), (100, 512))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_tokens_ids.shape, test_tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  2466,  1997,  1037,  2158,  2040,  2038, 21242,  5346,\n",
       "        2005,  1037, 10369,  1012,  4627,  2041,  2007,  1037,  3098,\n",
       "        3496,  2008,  2003,  1037, 27547,  2742,  1997, 18691,  4038,\n",
       "        1012,  1037,  5337,  4032,  4378,  2003,  2357,  2046,  2019,\n",
       "        9577,  1010,  6355, 11240,  2011,  1996,  4689, 22417,  2015,\n",
       "        1997,  2009,  1005,  1055,  8453,  1012,  6854,  2009, 12237,\n",
       "       18691,  1996,  2878,  2051,  2007,  2053,  2236,  7984,  2776,\n",
       "        2437,  2009,  2074,  2205,  2125,  5128,  1012,  2130,  2216,\n",
       "        2013,  1996,  3690,  2323,  2022,  2357,  2125,  1012,  1996,\n",
       "       26483,  7982,  2052,  2191,  8101,  4025,  3733,  2000,  1037,\n",
       "        2353,  3694,  2099,  1012,  2006,  1037,  4087,  2504,  2009,\n",
       "        1005,  1055,  2488,  2084,  2017,  2453,  2228,  2007,  2070,\n",
       "        2204, 16434,  2011,  2925,  2307,  6819, 13728,  2891,  1062,\n",
       "        5332, 21693, 15422,  1012,  2925,  3340,  8836, 11332,  3122,\n",
       "        1998, 15296, 16319,  2064,  2022,  2464,  4780,  1012,   102,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable is currently a list of neg and pos strings. We’ll convert it to numpy arrays of booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F7POtHuIOV-6",
    "outputId": "a5447340-6da3-4f2c-d284-098c4c906c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (100,), 0.5, 0.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.array(train_labels) != 'neg'\n",
    "test_y = np.array(test_labels) != 'neg'\n",
    "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-xXMEqXOWTE"
   },
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2K4JQMFo1-_S"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdzjl_WlwpKr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9jyb-hJ0xAgG",
    "outputId": "4cc5ca8c-646f-410f-d5c5-998fdf82f00e"
   },
   "outputs": [],
   "source": [
    "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9IzjAX_2VLf"
   },
   "outputs": [],
   "source": [
    "baseline_predicted = baseline_model.predict(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "QnsCRIaQ3GPQ",
    "outputId": "46417074-2870-4a52-9a6a-307bde86b704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.58      0.70        50\n",
      "        True       0.69      0.92      0.79        50\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.78      0.75      0.74       100\n",
      "weighted avg       0.78      0.75      0.74       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, baseline_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_hEhebQ3YqI"
   },
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E234ByBa3Qtb"
   },
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sf9n8zouENRi"
   },
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2466, 1997, ...,    0,    0,    0],\n",
       "       [ 101, 3199, 1005, ..., 2004, 2172,  102],\n",
       "       [ 101, 2023, 2143, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LOQ-870M7VWy",
    "outputId": "f7c010bf-eb70-413f-96db-c0eb14769cfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(train_tokens_ids[:3])\n",
    "y, pooled = bert_clf.bert(x)\n",
    "x.shape, y.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LCb_pK4X7hb9",
    "outputId": "77237549-1b51-4d51-a11e-84e53fa82cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6004767 ],\n",
       "       [0.68869555],\n",
       "       [0.62791777]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bert_clf(x)\n",
    "y.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9LPIYcn99r8"
   },
   "source": [
    "# Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ED9SE1Ka8W9x",
    "outputId": "ab8b549b-137b-491a-cf6f-f20718dd73ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUkXhM1k_TAl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGwV0yqg_o2u",
    "outputId": "236eae60-f405-4f2b-89be-1164fb924b91"
   },
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Yl2JpCe9YAu"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JF_QD0naS8EQ"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b28PcoDh_cyd"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertBinaryClassifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_clf = bert_clf.to(device)\n",
    "bert_clf = nn.DataParallel(bert_clf)\n",
    "bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mqh8tCl4AFjo",
    "outputId": "15c746cf-ba17-483a-d0ff-0bcbf9e96431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/250.0 loss: 0.9797638058662415 \n",
      "1/250.0 loss: 0.7473504841327667 \n",
      "2/250.0 loss: 0.682931919892629 \n",
      "3/250.0 loss: 0.7297441512346268 \n",
      "4/250.0 loss: 0.7674580454826355 \n",
      "5/250.0 loss: 0.7653777996699015 \n",
      "6/250.0 loss: 0.7358746613774981 \n",
      "7/250.0 loss: 0.7303956672549248 \n",
      "8/250.0 loss: 0.708951559331682 \n",
      "9/250.0 loss: 0.6826391637325286 \n",
      "10/250.0 loss: 0.6871807304295626 \n",
      "11/250.0 loss: 0.6901405900716782 \n",
      "12/250.0 loss: 0.7011182078948388 \n",
      "13/250.0 loss: 0.6919980006558555 \n",
      "14/250.0 loss: 0.6885264158248902 \n",
      "15/250.0 loss: 0.7005675919353962 \n",
      "16/250.0 loss: 0.6987220574827755 \n",
      "17/250.0 loss: 0.7049259576532576 \n",
      "18/250.0 loss: 0.705495464174371 \n",
      "19/250.0 loss: 0.7001570165157318 \n",
      "20/250.0 loss: 0.702959574404217 \n",
      "21/250.0 loss: 0.7088658836754885 \n",
      "22/250.0 loss: 0.7040618113849474 \n",
      "23/250.0 loss: 0.701962985098362 \n",
      "24/250.0 loss: 0.7022914433479309 \n",
      "25/250.0 loss: 0.7020295147712414 \n",
      "26/250.0 loss: 0.6958229563854359 \n",
      "27/250.0 loss: 0.6935213484934398 \n",
      "28/250.0 loss: 0.6928474553700151 \n",
      "29/250.0 loss: 0.6884624699751536 \n",
      "30/250.0 loss: 0.6896425043382952 \n",
      "31/250.0 loss: 0.6894049011170864 \n",
      "32/250.0 loss: 0.6933762900757067 \n",
      "33/250.0 loss: 0.6936988146866069 \n",
      "34/250.0 loss: 0.6983840465545654 \n",
      "35/250.0 loss: 0.6959068642722236 \n",
      "36/250.0 loss: 0.6952605585794192 \n",
      "37/250.0 loss: 0.6945689179395375 \n",
      "38/250.0 loss: 0.6953708101541568 \n",
      "39/250.0 loss: 0.6965908885002137 \n",
      "40/250.0 loss: 0.6962413380785686 \n",
      "41/250.0 loss: 0.6959027080308824 \n",
      "42/250.0 loss: 0.696181925230248 \n",
      "43/250.0 loss: 0.6960279684175145 \n",
      "44/250.0 loss: 0.7009953141212464 \n",
      "45/250.0 loss: 0.7002123192600582 \n",
      "46/250.0 loss: 0.6987517881900707 \n",
      "47/250.0 loss: 0.699177456398805 \n",
      "48/250.0 loss: 0.6977823096878675 \n",
      "49/250.0 loss: 0.693993011713028 \n",
      "50/250.0 loss: 0.6945046572124257 \n",
      "51/250.0 loss: 0.6952205472267591 \n",
      "52/250.0 loss: 0.6930110881913383 \n",
      "53/250.0 loss: 0.6925360891554091 \n",
      "54/250.0 loss: 0.6912777109579606 \n",
      "55/250.0 loss: 0.692975110241345 \n",
      "56/250.0 loss: 0.6924546015890021 \n",
      "57/250.0 loss: 0.6921719528477768 \n",
      "58/250.0 loss: 0.6936181642241397 \n",
      "59/250.0 loss: 0.692242231965065 \n",
      "60/250.0 loss: 0.6905143720204713 \n",
      "61/250.0 loss: 0.6897915784389742 \n",
      "62/250.0 loss: 0.6910100068364825 \n",
      "63/250.0 loss: 0.6895981086418033 \n",
      "64/250.0 loss: 0.6884637337464553 \n",
      "65/250.0 loss: 0.6887169733191981 \n",
      "66/250.0 loss: 0.6879730954099057 \n",
      "67/250.0 loss: 0.6862082341138054 \n",
      "68/250.0 loss: 0.6854873049086418 \n",
      "69/250.0 loss: 0.6847998125212533 \n",
      "70/250.0 loss: 0.685113743996956 \n",
      "71/250.0 loss: 0.6847841971450381 \n",
      "72/250.0 loss: 0.683574475654184 \n",
      "73/250.0 loss: 0.6831701349567723 \n",
      "74/250.0 loss: 0.6832703598340353 \n",
      "75/250.0 loss: 0.6821227395220807 \n",
      "76/250.0 loss: 0.6822171033202827 \n",
      "77/250.0 loss: 0.6813555321632287 \n",
      "78/250.0 loss: 0.6817386799220797 \n",
      "79/250.0 loss: 0.6797236308455468 \n",
      "80/250.0 loss: 0.6793777353969621 \n",
      "81/250.0 loss: 0.6791512806241106 \n",
      "82/250.0 loss: 0.6768154085400593 \n",
      "83/250.0 loss: 0.6768679576260703 \n",
      "84/250.0 loss: 0.6771298618877635 \n",
      "85/250.0 loss: 0.6762970672097317 \n",
      "86/250.0 loss: 0.6761639734794354 \n",
      "87/250.0 loss: 0.6761347908865322 \n",
      "88/250.0 loss: 0.6776753858234106 \n",
      "89/250.0 loss: 0.6773370775911544 \n",
      "90/250.0 loss: 0.6775563084162198 \n",
      "91/250.0 loss: 0.6765441862137421 \n",
      "92/250.0 loss: 0.6777233994135292 \n",
      "93/250.0 loss: 0.6806019170487181 \n",
      "94/250.0 loss: 0.6788580449003923 \n",
      "95/250.0 loss: 0.6784258335828781 \n",
      "96/250.0 loss: 0.6802689359360135 \n",
      "97/250.0 loss: 0.6802597246607955 \n",
      "98/250.0 loss: 0.6788785000040074 \n",
      "99/250.0 loss: 0.6789342224597931 \n",
      "100/250.0 loss: 0.6793191255909381 \n",
      "101/250.0 loss: 0.6803181749932906 \n",
      "102/250.0 loss: 0.6792368020826173 \n",
      "103/250.0 loss: 0.6785041563785993 \n",
      "104/250.0 loss: 0.6790758615448361 \n",
      "105/250.0 loss: 0.680208387802232 \n",
      "106/250.0 loss: 0.679587469479748 \n",
      "107/250.0 loss: 0.6802261295141997 \n",
      "108/250.0 loss: 0.6810387022998354 \n",
      "109/250.0 loss: 0.6795386265624653 \n",
      "110/250.0 loss: 0.6789061400267455 \n",
      "111/250.0 loss: 0.6794411412307194 \n",
      "112/250.0 loss: 0.6792304547487107 \n",
      "113/250.0 loss: 0.6791295341232366 \n",
      "114/250.0 loss: 0.6779155389122341 \n",
      "115/250.0 loss: 0.6767676071874027 \n",
      "116/250.0 loss: 0.6761288286274315 \n",
      "117/250.0 loss: 0.6780981603315321 \n",
      "118/250.0 loss: 0.6784866367067609 \n",
      "119/250.0 loss: 0.6775468518336614 \n",
      "120/250.0 loss: 0.6769449070465466 \n",
      "121/250.0 loss: 0.6765935186479912 \n",
      "122/250.0 loss: 0.6770265092694663 \n",
      "123/250.0 loss: 0.6777923453238702 \n",
      "124/250.0 loss: 0.6778227519989014 \n",
      "125/250.0 loss: 0.67641557303686 \n",
      "126/250.0 loss: 0.6758520335663022 \n",
      "127/250.0 loss: 0.6775773647241294 \n",
      "128/250.0 loss: 0.6761826512425445 \n",
      "129/250.0 loss: 0.6758364512370183 \n",
      "130/250.0 loss: 0.6747187931119031 \n",
      "131/250.0 loss: 0.6749004396525297 \n",
      "132/250.0 loss: 0.6739419909348165 \n",
      "133/250.0 loss: 0.6735072229335557 \n",
      "134/250.0 loss: 0.6738256918059455 \n",
      "135/250.0 loss: 0.6737327045377564 \n",
      "136/250.0 loss: 0.6730511593122552 \n",
      "137/250.0 loss: 0.6728907059068265 \n",
      "138/250.0 loss: 0.6722583642108835 \n",
      "139/250.0 loss: 0.673209308726447 \n",
      "140/250.0 loss: 0.6738187953935447 \n",
      "141/250.0 loss: 0.6736268984599852 \n",
      "142/250.0 loss: 0.6748346903107383 \n",
      "143/250.0 loss: 0.6735785009546412 \n",
      "144/250.0 loss: 0.6728839555691029 \n",
      "145/250.0 loss: 0.6724720240047534 \n",
      "146/250.0 loss: 0.6729433368257924 \n",
      "147/250.0 loss: 0.6731141888209291 \n",
      "148/250.0 loss: 0.6722812270558121 \n",
      "149/250.0 loss: 0.6715279136101405 \n",
      "150/250.0 loss: 0.6703214477624325 \n",
      "151/250.0 loss: 0.669170294153063 \n",
      "152/250.0 loss: 0.6686661851172354 \n",
      "153/250.0 loss: 0.669513887786246 \n",
      "154/250.0 loss: 0.668552440212619 \n",
      "155/250.0 loss: 0.6685847915135897 \n",
      "156/250.0 loss: 0.6682282450852121 \n",
      "157/250.0 loss: 0.66707712864574 \n",
      "158/250.0 loss: 0.6676612575099153 \n",
      "159/250.0 loss: 0.6667990427464247 \n",
      "160/250.0 loss: 0.6665868103874396 \n",
      "161/250.0 loss: 0.66710004651988 \n",
      "162/250.0 loss: 0.666779669150253 \n",
      "163/250.0 loss: 0.6667594717043203 \n",
      "164/250.0 loss: 0.6659998767303698 \n",
      "165/250.0 loss: 0.6653287561543016 \n",
      "166/250.0 loss: 0.6655180704093979 \n",
      "167/250.0 loss: 0.6640281560165542 \n",
      "168/250.0 loss: 0.6634000321111736 \n",
      "169/250.0 loss: 0.6627670757910784 \n",
      "170/250.0 loss: 0.6633284220918577 \n",
      "171/250.0 loss: 0.6634135554696239 \n",
      "172/250.0 loss: 0.663306740322554 \n",
      "173/250.0 loss: 0.66364216085138 \n",
      "174/250.0 loss: 0.664018657207489 \n",
      "175/250.0 loss: 0.6639834588224237 \n",
      "176/250.0 loss: 0.6626984766647641 \n",
      "177/250.0 loss: 0.6621506411707803 \n",
      "178/250.0 loss: 0.6614242988591753 \n",
      "179/250.0 loss: 0.6606256299548678 \n",
      "180/250.0 loss: 0.6614864662865907 \n",
      "181/250.0 loss: 0.6624763424579914 \n",
      "182/250.0 loss: 0.6618320583645764 \n",
      "183/250.0 loss: 0.661749817430973 \n",
      "184/250.0 loss: 0.6612354655523558 \n",
      "185/250.0 loss: 0.6628720212367273 \n",
      "186/250.0 loss: 0.6616161274399986 \n",
      "187/250.0 loss: 0.6611578286962306 \n",
      "188/250.0 loss: 0.6605642756457051 \n",
      "189/250.0 loss: 0.6610561819452988 \n",
      "190/250.0 loss: 0.6612800378449925 \n",
      "191/250.0 loss: 0.661074255903562 \n",
      "192/250.0 loss: 0.6613377789759265 \n",
      "193/250.0 loss: 0.6619273144559762 \n",
      "194/250.0 loss: 0.6622247356634874 \n",
      "195/250.0 loss: 0.6616091299421933 \n",
      "196/250.0 loss: 0.6618258151306113 \n",
      "197/250.0 loss: 0.6624997574271578 \n",
      "198/250.0 loss: 0.6625169356863703 \n",
      "199/250.0 loss: 0.6627835789322853 \n",
      "200/250.0 loss: 0.6626121849562991 \n",
      "201/250.0 loss: 0.6628447897953562 \n",
      "202/250.0 loss: 0.6633702425533915 \n",
      "203/250.0 loss: 0.6628891700038723 \n",
      "204/250.0 loss: 0.6632596088618767 \n",
      "205/250.0 loss: 0.6642216432441785 \n",
      "206/250.0 loss: 0.664291997462655 \n",
      "207/250.0 loss: 0.6638443836799035 \n",
      "208/250.0 loss: 0.6634482236570148 \n",
      "209/250.0 loss: 0.6628748280661446 \n",
      "210/250.0 loss: 0.6629082230030078 \n",
      "211/250.0 loss: 0.6645639741195822 \n",
      "212/250.0 loss: 0.6650886888235388 \n",
      "213/250.0 loss: 0.6643412444636086 \n",
      "214/250.0 loss: 0.6637895980546641 \n",
      "215/250.0 loss: 0.6641425907059952 \n",
      "216/250.0 loss: 0.6640117737005383 \n",
      "217/250.0 loss: 0.6640860908622042 \n",
      "218/250.0 loss: 0.6634528223238035 \n",
      "219/250.0 loss: 0.6625246049328284 \n",
      "220/250.0 loss: 0.6636215438939866 \n",
      "221/250.0 loss: 0.6626336832304258 \n",
      "222/250.0 loss: 0.6620519524732513 \n",
      "223/250.0 loss: 0.6634471541536706 \n",
      "224/250.0 loss: 0.6631510596805149 \n",
      "225/250.0 loss: 0.6626034015047867 \n",
      "226/250.0 loss: 0.6615217371396557 \n",
      "227/250.0 loss: 0.6615460644427099 \n",
      "228/250.0 loss: 0.6609782301963156 \n",
      "229/250.0 loss: 0.66249078810215 \n",
      "230/250.0 loss: 0.6616423306785104 \n",
      "231/250.0 loss: 0.6630326405424496 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/250.0 loss: 0.6622553269494756 \n",
      "233/250.0 loss: 0.6621223979780817 \n",
      "234/250.0 loss: 0.6626566959188339 \n",
      "235/250.0 loss: 0.6624896350806042 \n",
      "236/250.0 loss: 0.6618388248646813 \n",
      "237/250.0 loss: 0.6616936793096927 \n",
      "238/250.0 loss: 0.6612029022003317 \n",
      "239/250.0 loss: 0.6602355220665534 \n",
      "240/250.0 loss: 0.6609301847788308 \n",
      "241/250.0 loss: 0.6607732168160194 \n",
      "242/250.0 loss: 0.6596623335355594 \n",
      "243/250.0 loss: 0.6601774841547012 \n",
      "244/250.0 loss: 0.6598543461488218 \n",
      "245/250.0 loss: 0.6591818075839097 \n",
      "246/250.0 loss: 0.6581241733390792 \n",
      "247/250.0 loss: 0.6575413155219247 \n",
      "248/250.0 loss: 0.657438865986215 \n",
      "249/250.0 loss: 0.6575364841222763 \n",
      "Epoch:  2\n",
      "0/250.0 loss: 0.4162587523460388 \n",
      "1/250.0 loss: 0.3888309895992279 \n",
      "2/250.0 loss: 0.39116912086804706 \n",
      "3/250.0 loss: 0.40637245774269104 \n",
      "4/250.0 loss: 0.46193733215332033 \n",
      "5/250.0 loss: 0.47094933191935223 \n",
      "6/250.0 loss: 0.449678157057081 \n",
      "7/250.0 loss: 0.4785948842763901 \n",
      "8/250.0 loss: 0.4769847293694814 \n",
      "9/250.0 loss: 0.4826521843671799 \n",
      "10/250.0 loss: 0.49377414042299445 \n",
      "11/250.0 loss: 0.48401252677043277 \n",
      "12/250.0 loss: 0.4872963268023271 \n",
      "13/250.0 loss: 0.493446769458907 \n",
      "14/250.0 loss: 0.5023150384426117 \n",
      "15/250.0 loss: 0.4992366209626198 \n",
      "16/250.0 loss: 0.4987529323381536 \n",
      "17/250.0 loss: 0.4990222007036209 \n",
      "18/250.0 loss: 0.4928236180230191 \n",
      "19/250.0 loss: 0.5051005944609642 \n",
      "20/250.0 loss: 0.5059514258589063 \n",
      "21/250.0 loss: 0.5093862834301862 \n",
      "22/250.0 loss: 0.5138830866502679 \n",
      "23/250.0 loss: 0.5135732827087244 \n",
      "24/250.0 loss: 0.5090530347824097 \n",
      "25/250.0 loss: 0.5117386900461637 \n",
      "26/250.0 loss: 0.5175170765982734 \n",
      "27/250.0 loss: 0.5150621959141323 \n",
      "28/250.0 loss: 0.5123970446915462 \n",
      "29/250.0 loss: 0.5118622620900471 \n",
      "30/250.0 loss: 0.5105970098126319 \n",
      "31/250.0 loss: 0.5167867448180914 \n",
      "32/250.0 loss: 0.5140486206069137 \n",
      "33/250.0 loss: 0.5137371149133233 \n",
      "34/250.0 loss: 0.5122953270162854 \n",
      "35/250.0 loss: 0.521395083102915 \n",
      "36/250.0 loss: 0.5187002782886093 \n",
      "37/250.0 loss: 0.5175499139647735 \n",
      "38/250.0 loss: 0.5242993472478329 \n",
      "39/250.0 loss: 0.5220529057085515 \n",
      "40/250.0 loss: 0.5230118784962631 \n",
      "41/250.0 loss: 0.5212991627908888 \n",
      "42/250.0 loss: 0.5228113123150759 \n",
      "43/250.0 loss: 0.5309715277769349 \n",
      "44/250.0 loss: 0.5274504880110423 \n",
      "45/250.0 loss: 0.5381225250337435 \n",
      "46/250.0 loss: 0.5410124501015278 \n",
      "47/250.0 loss: 0.5462079104036093 \n",
      "48/250.0 loss: 0.5469815250562162 \n",
      "49/250.0 loss: 0.5454976588487626 \n",
      "50/250.0 loss: 0.5445851923203936 \n",
      "51/250.0 loss: 0.5436232766279807 \n",
      "52/250.0 loss: 0.5446374112705015 \n",
      "53/250.0 loss: 0.5447466439670987 \n",
      "54/250.0 loss: 0.5445945382118225 \n",
      "55/250.0 loss: 0.5416576181139264 \n",
      "56/250.0 loss: 0.5378861662588621 \n",
      "57/250.0 loss: 0.5417889721434692 \n",
      "58/250.0 loss: 0.540986456103244 \n",
      "59/250.0 loss: 0.5405636837085088 \n",
      "60/250.0 loss: 0.5410327100362934 \n",
      "61/250.0 loss: 0.5456474529158685 \n",
      "62/250.0 loss: 0.5429003910412864 \n",
      "63/250.0 loss: 0.5447404766455293 \n",
      "64/250.0 loss: 0.546886226764092 \n",
      "65/250.0 loss: 0.5440144895604162 \n",
      "66/250.0 loss: 0.544647506368694 \n",
      "67/250.0 loss: 0.546773864504169 \n",
      "68/250.0 loss: 0.543956733700158 \n",
      "69/250.0 loss: 0.5449521401098796 \n",
      "70/250.0 loss: 0.5422348904777581 \n",
      "71/250.0 loss: 0.5410786846445667 \n",
      "72/250.0 loss: 0.5386436569364104 \n",
      "73/250.0 loss: 0.5417895997698242 \n",
      "74/250.0 loss: 0.544053616921107 \n",
      "75/250.0 loss: 0.542283826360577 \n",
      "76/250.0 loss: 0.5407140351735152 \n",
      "77/250.0 loss: 0.5456526451385938 \n",
      "78/250.0 loss: 0.5491425919381878 \n",
      "79/250.0 loss: 0.5497463438659906 \n",
      "80/250.0 loss: 0.5473448777640307 \n",
      "81/250.0 loss: 0.5454850298602406 \n",
      "82/250.0 loss: 0.548802326960736 \n",
      "83/250.0 loss: 0.5488133175032479 \n",
      "84/250.0 loss: 0.5473732106825885 \n",
      "85/250.0 loss: 0.5522089240162872 \n",
      "86/250.0 loss: 0.5510777238456682 \n",
      "87/250.0 loss: 0.5514981370757926 \n",
      "88/250.0 loss: 0.5516052430265406 \n",
      "89/250.0 loss: 0.5524537420935101 \n",
      "90/250.0 loss: 0.5528779937015785 \n",
      "91/250.0 loss: 0.5523828672974006 \n",
      "92/250.0 loss: 0.5514691879031479 \n",
      "93/250.0 loss: 0.5523930651710388 \n",
      "94/250.0 loss: 0.5517797561068284 \n",
      "95/250.0 loss: 0.5515032432352504 \n",
      "96/250.0 loss: 0.5493227679704883 \n",
      "97/250.0 loss: 0.546993361139784 \n",
      "98/250.0 loss: 0.5462788207964464 \n",
      "99/250.0 loss: 0.543557545542717 \n",
      "100/250.0 loss: 0.542086655550664 \n",
      "101/250.0 loss: 0.5409869729888206 \n",
      "102/250.0 loss: 0.5399007325612225 \n",
      "103/250.0 loss: 0.5392447116856391 \n",
      "104/250.0 loss: 0.5380896968500954 \n",
      "105/250.0 loss: 0.535591076286334 \n",
      "106/250.0 loss: 0.5337394938290676 \n",
      "107/250.0 loss: 0.5343642074752737 \n",
      "108/250.0 loss: 0.534384871294739 \n",
      "109/250.0 loss: 0.5358162560246208 \n",
      "110/250.0 loss: 0.535482792703955 \n",
      "111/250.0 loss: 0.5357964661504541 \n",
      "112/250.0 loss: 0.5373276002639162 \n",
      "113/250.0 loss: 0.535948927465238 \n",
      "114/250.0 loss: 0.5355752553628839 \n",
      "115/250.0 loss: 0.5345883664899859 \n",
      "116/250.0 loss: 0.5351445978013878 \n",
      "117/250.0 loss: 0.5354734889026416 \n",
      "118/250.0 loss: 0.5356617141671541 \n",
      "119/250.0 loss: 0.5368951715528965 \n",
      "120/250.0 loss: 0.5351425682710222 \n",
      "121/250.0 loss: 0.5363081290585096 \n",
      "122/250.0 loss: 0.5360225829167095 \n",
      "123/250.0 loss: 0.536859849527959 \n",
      "124/250.0 loss: 0.5358276679515839 \n",
      "125/250.0 loss: 0.5385162345473729 \n",
      "126/250.0 loss: 0.5397907699656299 \n",
      "127/250.0 loss: 0.5408143440727144 \n",
      "128/250.0 loss: 0.5388583187447038 \n",
      "129/250.0 loss: 0.538264467395269 \n",
      "130/250.0 loss: 0.5374705429750545 \n",
      "131/250.0 loss: 0.5356403993386211 \n",
      "132/250.0 loss: 0.5363952434600744 \n",
      "133/250.0 loss: 0.5381339652769601 \n",
      "134/250.0 loss: 0.53900085400652 \n",
      "135/250.0 loss: 0.538316424278652 \n",
      "136/250.0 loss: 0.5400588934438942 \n",
      "137/250.0 loss: 0.5396110143350519 \n",
      "138/250.0 loss: 0.5398095683228198 \n",
      "139/250.0 loss: 0.539309235555785 \n",
      "140/250.0 loss: 0.54025799036026 \n",
      "141/250.0 loss: 0.5398675602086833 \n",
      "142/250.0 loss: 0.5387368879534982 \n",
      "143/250.0 loss: 0.5392640731814835 \n",
      "144/250.0 loss: 0.5379618007561257 \n",
      "145/250.0 loss: 0.5367186904361804 \n",
      "146/250.0 loss: 0.5379876155837052 \n",
      "147/250.0 loss: 0.5368694900660902 \n",
      "148/250.0 loss: 0.5356172503240957 \n",
      "149/250.0 loss: 0.5354832422733307 \n",
      "150/250.0 loss: 0.5352381673080242 \n",
      "151/250.0 loss: 0.5384541820538672 \n",
      "152/250.0 loss: 0.5368364963656157 \n",
      "153/250.0 loss: 0.5375721477842951 \n",
      "154/250.0 loss: 0.539176417550733 \n",
      "155/250.0 loss: 0.5393789315070862 \n",
      "156/250.0 loss: 0.5387967304818949 \n",
      "157/250.0 loss: 0.5405781427516213 \n",
      "158/250.0 loss: 0.5408884226900976 \n",
      "159/250.0 loss: 0.5394298307597637 \n",
      "160/250.0 loss: 0.5386503464686945 \n",
      "161/250.0 loss: 0.5406817532615897 \n",
      "162/250.0 loss: 0.540381529214192 \n",
      "163/250.0 loss: 0.5401456214305831 \n",
      "164/250.0 loss: 0.5407637578068357 \n",
      "165/250.0 loss: 0.5395194020975067 \n",
      "166/250.0 loss: 0.541592090251203 \n",
      "167/250.0 loss: 0.5408936116312232 \n",
      "168/250.0 loss: 0.5414167369258475 \n",
      "169/250.0 loss: 0.5414978989783455 \n",
      "170/250.0 loss: 0.5414117210441165 \n",
      "171/250.0 loss: 0.5399300792189532 \n",
      "172/250.0 loss: 0.5410032220658539 \n",
      "173/250.0 loss: 0.5413335760434469 \n",
      "174/250.0 loss: 0.5402623479706901 \n",
      "175/250.0 loss: 0.5392645981840112 \n",
      "176/250.0 loss: 0.5389868285359636 \n",
      "177/250.0 loss: 0.5386309633763988 \n",
      "178/250.0 loss: 0.5367905166229057 \n",
      "179/250.0 loss: 0.5355474334624079 \n",
      "180/250.0 loss: 0.5353084479906283 \n",
      "181/250.0 loss: 0.5373441396833776 \n",
      "182/250.0 loss: 0.53740956255647 \n",
      "183/250.0 loss: 0.5374388218573902 \n",
      "184/250.0 loss: 0.5358172151687983 \n",
      "185/250.0 loss: 0.5359208307439282 \n",
      "186/250.0 loss: 0.5357055856104203 \n",
      "187/250.0 loss: 0.5362672511725983 \n",
      "188/250.0 loss: 0.5351902958419588 \n",
      "189/250.0 loss: 0.5346818803956634 \n",
      "190/250.0 loss: 0.535509689238059 \n",
      "191/250.0 loss: 0.5341736055755367 \n",
      "192/250.0 loss: 0.5334423653047937 \n",
      "193/250.0 loss: 0.5320135604628583 \n",
      "194/250.0 loss: 0.5307315098169523 \n",
      "195/250.0 loss: 0.5312114624800731 \n",
      "196/250.0 loss: 0.5320600718259811 \n",
      "197/250.0 loss: 0.5305924916056671 \n",
      "198/250.0 loss: 0.5308412053626985 \n",
      "199/250.0 loss: 0.5295412626117468 \n",
      "200/250.0 loss: 0.5312791219842967 \n",
      "201/250.0 loss: 0.5309181504762999 \n",
      "202/250.0 loss: 0.5298145712156014 \n",
      "203/250.0 loss: 0.529489513340534 \n",
      "204/250.0 loss: 0.5279508932334621 \n",
      "205/250.0 loss: 0.5274193189965869 \n",
      "206/250.0 loss: 0.5298160523320166 \n",
      "207/250.0 loss: 0.52879421126384 \n",
      "208/250.0 loss: 0.5275224636901509 \n",
      "209/250.0 loss: 0.5262313466696512 \n",
      "210/250.0 loss: 0.5259083245320343 \n",
      "211/250.0 loss: 0.5245803046338963 \n",
      "212/250.0 loss: 0.523116055578693 \n",
      "213/250.0 loss: 0.522654286263702 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/250.0 loss: 0.5217100960570712 \n",
      "215/250.0 loss: 0.5213259902127363 \n",
      "216/250.0 loss: 0.5233627933373649 \n",
      "217/250.0 loss: 0.5233349968944121 \n",
      "218/250.0 loss: 0.5220680899543849 \n",
      "219/250.0 loss: 0.5243619261817498 \n",
      "220/250.0 loss: 0.5284934015565328 \n",
      "221/250.0 loss: 0.527969623739655 \n",
      "222/250.0 loss: 0.5288959851179422 \n",
      "223/250.0 loss: 0.528619778342545 \n",
      "224/250.0 loss: 0.5270292886098226 \n",
      "225/250.0 loss: 0.5258541916851449 \n",
      "226/250.0 loss: 0.5263085633122448 \n",
      "227/250.0 loss: 0.5268909703744086 \n",
      "228/250.0 loss: 0.5254884550925426 \n",
      "229/250.0 loss: 0.5242591704363408 \n",
      "230/250.0 loss: 0.5229185131334123 \n",
      "231/250.0 loss: 0.5219710999136341 \n",
      "232/250.0 loss: 0.5232894549170277 \n",
      "233/250.0 loss: 0.5226701870560646 \n",
      "234/250.0 loss: 0.5210972704785936 \n",
      "235/250.0 loss: 0.5197333320975304 \n",
      "236/250.0 loss: 0.5197779182894824 \n",
      "237/250.0 loss: 0.518574356907556 \n",
      "238/250.0 loss: 0.5212759965383856 \n",
      "239/250.0 loss: 0.5203671641647816 \n",
      "240/250.0 loss: 0.5193378325567206 \n",
      "241/250.0 loss: 0.5196353727874677 \n",
      "242/250.0 loss: 0.5191714470523866 \n",
      "243/250.0 loss: 0.5198332732085322 \n",
      "244/250.0 loss: 0.519566329887935 \n",
      "245/250.0 loss: 0.5207457009369765 \n",
      "246/250.0 loss: 0.5214516228509818 \n",
      "247/250.0 loss: 0.5208288782065914 \n",
      "248/250.0 loss: 0.5204106459177162 \n",
      "249/250.0 loss: 0.5200240178108215 \n",
      "Epoch:  3\n",
      "0/250.0 loss: 0.25362032651901245 \n",
      "1/250.0 loss: 0.35694803297519684 \n",
      "2/250.0 loss: 0.3221373160680135 \n",
      "3/250.0 loss: 0.4083593487739563 \n",
      "4/250.0 loss: 0.37900863885879515 \n",
      "5/250.0 loss: 0.41701937715212506 \n",
      "6/250.0 loss: 0.3893083355256489 \n",
      "7/250.0 loss: 0.3572436738759279 \n",
      "8/250.0 loss: 0.3636367900504006 \n",
      "9/250.0 loss: 0.35314851552248 \n",
      "10/250.0 loss: 0.3455578481609171 \n",
      "11/250.0 loss: 0.38064807280898094 \n",
      "12/250.0 loss: 0.4016765642624635 \n",
      "13/250.0 loss: 0.3925855574863298 \n",
      "14/250.0 loss: 0.40239979326725006 \n",
      "15/250.0 loss: 0.4106307802721858 \n",
      "16/250.0 loss: 0.42705385386943817 \n",
      "17/250.0 loss: 0.4122238639328215 \n",
      "18/250.0 loss: 0.41970202640483256 \n",
      "19/250.0 loss: 0.407919679582119 \n",
      "20/250.0 loss: 0.4099741961274828 \n",
      "21/250.0 loss: 0.4128178588368676 \n",
      "22/250.0 loss: 0.42606266555578814 \n",
      "23/250.0 loss: 0.42112801348169643 \n",
      "24/250.0 loss: 0.4356321465969086 \n",
      "25/250.0 loss: 0.42605902369205767 \n",
      "26/250.0 loss: 0.41643058315471365 \n",
      "27/250.0 loss: 0.41226508202297346 \n",
      "28/250.0 loss: 0.436297275382897 \n",
      "29/250.0 loss: 0.45315164774656297 \n",
      "30/250.0 loss: 0.4560575528490928 \n",
      "31/250.0 loss: 0.4602711987681687 \n",
      "32/250.0 loss: 0.4528804769118627 \n",
      "33/250.0 loss: 0.44536574973779564 \n",
      "34/250.0 loss: 0.448871978691646 \n",
      "35/250.0 loss: 0.4507738798856735 \n",
      "36/250.0 loss: 0.45221804767041596 \n",
      "37/250.0 loss: 0.44598985502594396 \n",
      "38/250.0 loss: 0.4448097149531047 \n",
      "39/250.0 loss: 0.43833683282136915 \n",
      "40/250.0 loss: 0.43544045308741125 \n",
      "41/250.0 loss: 0.44177703630356563 \n",
      "42/250.0 loss: 0.4362273500409237 \n",
      "43/250.0 loss: 0.4303957565941594 \n",
      "44/250.0 loss: 0.4274115459786521 \n",
      "45/250.0 loss: 0.4363665648776552 \n",
      "46/250.0 loss: 0.43281301000016803 \n",
      "47/250.0 loss: 0.4282179024691383 \n",
      "48/250.0 loss: 0.4267393177261158 \n",
      "49/250.0 loss: 0.4288985523581505 \n",
      "50/250.0 loss: 0.4337161145373887 \n",
      "51/250.0 loss: 0.437583862302395 \n",
      "52/250.0 loss: 0.45054768250798277 \n",
      "53/250.0 loss: 0.4458160193430053 \n",
      "54/250.0 loss: 0.443982669711113 \n",
      "55/250.0 loss: 0.44449858979455065 \n",
      "56/250.0 loss: 0.44213356966512246 \n",
      "57/250.0 loss: 0.4406488278302653 \n",
      "58/250.0 loss: 0.43679505106756245 \n",
      "59/250.0 loss: 0.44450057968497275 \n",
      "60/250.0 loss: 0.43958414162768694 \n",
      "61/250.0 loss: 0.44041261773916984 \n",
      "62/250.0 loss: 0.4426389178113332 \n",
      "63/250.0 loss: 0.44820320256985724 \n",
      "64/250.0 loss: 0.4504317515171491 \n",
      "65/250.0 loss: 0.44824250652031467 \n",
      "66/250.0 loss: 0.4445900105273546 \n",
      "67/250.0 loss: 0.44430440510897073 \n",
      "68/250.0 loss: 0.4404416185790214 \n",
      "69/250.0 loss: 0.44301418044737406 \n",
      "70/250.0 loss: 0.43874525352263116 \n",
      "71/250.0 loss: 0.4395648005108039 \n",
      "72/250.0 loss: 0.43599789885625445 \n",
      "73/250.0 loss: 0.4381717342782665 \n",
      "74/250.0 loss: 0.44162973364194236 \n",
      "75/250.0 loss: 0.4403489914379622 \n",
      "76/250.0 loss: 0.44277253011604406 \n",
      "77/250.0 loss: 0.44058147684121746 \n",
      "78/250.0 loss: 0.4465518329716936 \n",
      "79/250.0 loss: 0.44866311475634574 \n",
      "80/250.0 loss: 0.44614698452714047 \n",
      "81/250.0 loss: 0.444848515638491 \n",
      "82/250.0 loss: 0.44159071291067514 \n",
      "83/250.0 loss: 0.43988804182126406 \n",
      "84/250.0 loss: 0.4442361763295005 \n",
      "85/250.0 loss: 0.4428287822493287 \n",
      "86/250.0 loss: 0.4408972056090147 \n",
      "87/250.0 loss: 0.4430966362018477 \n",
      "88/250.0 loss: 0.4399012275291293 \n",
      "89/250.0 loss: 0.43665394286314646 \n",
      "90/250.0 loss: 0.43708340512527216 \n",
      "91/250.0 loss: 0.4372323391878087 \n",
      "92/250.0 loss: 0.43479204946948635 \n",
      "93/250.0 loss: 0.4343135401289514 \n",
      "94/250.0 loss: 0.4317541558491556 \n",
      "95/250.0 loss: 0.4318351624533534 \n",
      "96/250.0 loss: 0.4349416853840818 \n",
      "97/250.0 loss: 0.44004568852940384 \n",
      "98/250.0 loss: 0.445525032402289 \n",
      "99/250.0 loss: 0.44863988071680067 \n",
      "100/250.0 loss: 0.4510492703112045 \n",
      "101/250.0 loss: 0.4479181512313731 \n",
      "102/250.0 loss: 0.44490146665897184 \n",
      "103/250.0 loss: 0.44258807900433356 \n",
      "104/250.0 loss: 0.4401116995584397 \n",
      "105/250.0 loss: 0.43778485650161525 \n",
      "106/250.0 loss: 0.43568104839770594 \n",
      "107/250.0 loss: 0.43362711949480903 \n",
      "108/250.0 loss: 0.43115023516733714 \n",
      "109/250.0 loss: 0.4286242019046437 \n",
      "110/250.0 loss: 0.42617548827652457 \n",
      "111/250.0 loss: 0.4275114773107426 \n",
      "112/250.0 loss: 0.4292479349976092 \n",
      "113/250.0 loss: 0.4271032831125092 \n",
      "114/250.0 loss: 0.4282183077024377 \n",
      "115/250.0 loss: 0.4281477825394992 \n",
      "116/250.0 loss: 0.4255340731678865 \n",
      "117/250.0 loss: 0.42315912455067795 \n",
      "118/250.0 loss: 0.42122076495605354 \n",
      "119/250.0 loss: 0.4188255501911044 \n",
      "120/250.0 loss: 0.4189652554875563 \n",
      "121/250.0 loss: 0.4167823440349493 \n",
      "122/250.0 loss: 0.4151485611389323 \n",
      "123/250.0 loss: 0.4142034592767877 \n",
      "124/250.0 loss: 0.4149158225655556 \n",
      "125/250.0 loss: 0.4125810425787691 \n",
      "126/250.0 loss: 0.41073357419470163 \n",
      "127/250.0 loss: 0.4088601869880222 \n",
      "128/250.0 loss: 0.40666535236807755 \n",
      "129/250.0 loss: 0.40737746375111433 \n",
      "130/250.0 loss: 0.4053177691939223 \n",
      "131/250.0 loss: 0.404118355194276 \n",
      "132/250.0 loss: 0.4062576111882253 \n",
      "133/250.0 loss: 0.40756492266681654 \n",
      "134/250.0 loss: 0.4061154128776656 \n",
      "135/250.0 loss: 0.40550272599520054 \n",
      "136/250.0 loss: 0.40802669389186985 \n",
      "137/250.0 loss: 0.4095995188083338 \n",
      "138/250.0 loss: 0.40962931679950343 \n",
      "139/250.0 loss: 0.4126478660851717 \n",
      "140/250.0 loss: 0.41106596351303953 \n",
      "141/250.0 loss: 0.41079993939525644 \n",
      "142/250.0 loss: 0.40939014748885083 \n",
      "143/250.0 loss: 0.41018341290247107 \n",
      "144/250.0 loss: 0.4084927331270843 \n",
      "145/250.0 loss: 0.40769840414597563 \n",
      "146/250.0 loss: 0.4055536444596693 \n",
      "147/250.0 loss: 0.4051475273696957 \n",
      "148/250.0 loss: 0.40321661066888964 \n",
      "149/250.0 loss: 0.4037450371682644 \n",
      "150/250.0 loss: 0.4024638958423343 \n",
      "151/250.0 loss: 0.4044134513425984 \n",
      "152/250.0 loss: 0.40392230718938354 \n",
      "153/250.0 loss: 0.405460806490926 \n",
      "154/250.0 loss: 0.40565793999741157 \n",
      "155/250.0 loss: 0.40404699117136306 \n",
      "156/250.0 loss: 0.40266017250384495 \n",
      "157/250.0 loss: 0.40232899469099465 \n",
      "158/250.0 loss: 0.4030157389974444 \n",
      "159/250.0 loss: 0.4027414525393397 \n",
      "160/250.0 loss: 0.4029949139456571 \n",
      "161/250.0 loss: 0.40154517676543305 \n",
      "162/250.0 loss: 0.4005676933501396 \n",
      "163/250.0 loss: 0.3989138613659434 \n",
      "164/250.0 loss: 0.39945327093203864 \n",
      "165/250.0 loss: 0.39850434674376467 \n",
      "166/250.0 loss: 0.4025732262316578 \n",
      "167/250.0 loss: 0.40358212780916974 \n",
      "168/250.0 loss: 0.40207456268326064 \n",
      "169/250.0 loss: 0.40049379656420037 \n",
      "170/250.0 loss: 0.4022832127255306 \n",
      "171/250.0 loss: 0.4031716669628093 \n",
      "172/250.0 loss: 0.40606232129597253 \n",
      "173/250.0 loss: 0.4062913152250065 \n",
      "174/250.0 loss: 0.4058736443093845 \n",
      "175/250.0 loss: 0.408117746045305 \n",
      "176/250.0 loss: 0.4127835721871947 \n",
      "177/250.0 loss: 0.4120961624835984 \n",
      "178/250.0 loss: 0.4112273680680957 \n",
      "179/250.0 loss: 0.4148315349800719 \n",
      "180/250.0 loss: 0.4141129249943554 \n",
      "181/250.0 loss: 0.4155060014800056 \n",
      "182/250.0 loss: 0.4141632487457958 \n",
      "183/250.0 loss: 0.4129683920789672 \n",
      "184/250.0 loss: 0.41299320407010415 \n",
      "185/250.0 loss: 0.41377858172661514 \n",
      "186/250.0 loss: 0.4139443004434121 \n",
      "187/250.0 loss: 0.41305880593334104 \n",
      "188/250.0 loss: 0.41388851116416314 \n",
      "189/250.0 loss: 0.4141556494722241 \n",
      "190/250.0 loss: 0.41326762724141175 \n",
      "191/250.0 loss: 0.41212845232803375 \n",
      "192/250.0 loss: 0.4139672945123262 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/250.0 loss: 0.41269460086201887 \n",
      "194/250.0 loss: 0.4131621640080061 \n",
      "195/250.0 loss: 0.4115607668550647 \n",
      "196/250.0 loss: 0.4105325787503102 \n",
      "197/250.0 loss: 0.4118577502291612 \n",
      "198/250.0 loss: 0.41175394845967317 \n",
      "199/250.0 loss: 0.4126826174557209 \n",
      "200/250.0 loss: 0.4163923413302768 \n",
      "201/250.0 loss: 0.4175869838731124 \n",
      "202/250.0 loss: 0.4160731315906412 \n",
      "203/250.0 loss: 0.4185016875465711 \n",
      "204/250.0 loss: 0.4174504034402894 \n",
      "205/250.0 loss: 0.4183234327915803 \n",
      "206/250.0 loss: 0.4186830650205198 \n",
      "207/250.0 loss: 0.41965197026729584 \n",
      "208/250.0 loss: 0.4197934520872016 \n",
      "209/250.0 loss: 0.41977596084276836 \n",
      "210/250.0 loss: 0.4240586172912923 \n",
      "211/250.0 loss: 0.42270343033772595 \n",
      "212/250.0 loss: 0.4224900485764087 \n",
      "213/250.0 loss: 0.4210998140902163 \n",
      "214/250.0 loss: 0.4198321484549101 \n",
      "215/250.0 loss: 0.41900744544411145 \n",
      "216/250.0 loss: 0.41907467634721834 \n",
      "217/250.0 loss: 0.4180545445292368 \n",
      "218/250.0 loss: 0.41718613291712114 \n",
      "219/250.0 loss: 0.41619388142769986 \n",
      "220/250.0 loss: 0.41504193990748395 \n",
      "221/250.0 loss: 0.41551556693272546 \n",
      "222/250.0 loss: 0.416777894034514 \n",
      "223/250.0 loss: 0.4171414700602846 \n",
      "224/250.0 loss: 0.4183284052875307 \n",
      "225/250.0 loss: 0.417747854724922 \n",
      "226/250.0 loss: 0.4164519466229998 \n",
      "227/250.0 loss: 0.41750387389931765 \n",
      "228/250.0 loss: 0.41637738667200747 \n",
      "229/250.0 loss: 0.4170676614927209 \n",
      "230/250.0 loss: 0.4176427623926303 \n",
      "231/250.0 loss: 0.4196895587033239 \n",
      "232/250.0 loss: 0.4185158040518413 \n",
      "233/250.0 loss: 0.41737062853370976 \n",
      "234/250.0 loss: 0.41810722446188014 \n",
      "235/250.0 loss: 0.41708023107405434 \n",
      "236/250.0 loss: 0.41741814033643104 \n",
      "237/250.0 loss: 0.4162289645491528 \n",
      "238/250.0 loss: 0.4155048081440906 \n",
      "239/250.0 loss: 0.4151669392362237 \n",
      "240/250.0 loss: 0.41405122390179216 \n",
      "241/250.0 loss: 0.4153199230959593 \n",
      "242/250.0 loss: 0.41439008436821123 \n",
      "243/250.0 loss: 0.41323333331307427 \n",
      "244/250.0 loss: 0.4121870971455866 \n",
      "245/250.0 loss: 0.4111921607963438 \n",
      "246/250.0 loss: 0.41191011618988715 \n",
      "247/250.0 loss: 0.4120026549264308 \n",
      "248/250.0 loss: 0.4115096006288107 \n",
      "249/250.0 loss: 0.4120461677312851 \n",
      "Epoch:  4\n",
      "0/250.0 loss: 0.6129980087280273 \n",
      "1/250.0 loss: 0.3824943006038666 \n",
      "2/250.0 loss: 0.314665029446284 \n",
      "3/250.0 loss: 0.31211087852716446 \n",
      "4/250.0 loss: 0.28111482560634615 \n",
      "5/250.0 loss: 0.258003128071626 \n",
      "6/250.0 loss: 0.3056879831211908 \n",
      "7/250.0 loss: 0.29605369083583355 \n",
      "8/250.0 loss: 0.28073768814404804 \n",
      "9/250.0 loss: 0.2646341219544411 \n",
      "10/250.0 loss: 0.25038480216806586 \n",
      "11/250.0 loss: 0.2383534579227368 \n",
      "12/250.0 loss: 0.23522024601697922 \n",
      "13/250.0 loss: 0.22762570370520865 \n",
      "14/250.0 loss: 0.24933908134698868 \n",
      "15/250.0 loss: 0.28903099289163947 \n",
      "16/250.0 loss: 0.2884697611717617 \n",
      "17/250.0 loss: 0.2821752267579238 \n",
      "18/250.0 loss: 0.2733467001664011 \n",
      "19/250.0 loss: 0.2674513772130013 \n",
      "20/250.0 loss: 0.2603724329244523 \n",
      "21/250.0 loss: 0.25545179031111975 \n",
      "22/250.0 loss: 0.24955951973148013 \n",
      "23/250.0 loss: 0.2600370328873396 \n",
      "24/250.0 loss: 0.26749297440052033 \n",
      "25/250.0 loss: 0.2819205184395497 \n",
      "26/250.0 loss: 0.28739165431923336 \n",
      "27/250.0 loss: 0.2929907265518393 \n",
      "28/250.0 loss: 0.2859086137393425 \n",
      "29/250.0 loss: 0.29200613796710967 \n",
      "30/250.0 loss: 0.30376544210218615 \n",
      "31/250.0 loss: 0.30170198157429695 \n",
      "32/250.0 loss: 0.295793715299982 \n",
      "33/250.0 loss: 0.30671543014400143 \n",
      "34/250.0 loss: 0.3006875740630286 \n",
      "35/250.0 loss: 0.2968112747702334 \n",
      "36/250.0 loss: 0.3113307457517933 \n",
      "37/250.0 loss: 0.3065445952509579 \n",
      "38/250.0 loss: 0.30198941972011173 \n",
      "39/250.0 loss: 0.3107051748782396 \n",
      "40/250.0 loss: 0.31751973781643844 \n",
      "41/250.0 loss: 0.3126575787152563 \n",
      "42/250.0 loss: 0.3310108139764431 \n",
      "43/250.0 loss: 0.3300155391069976 \n",
      "44/250.0 loss: 0.3477859354681439 \n",
      "45/250.0 loss: 0.3530389993734982 \n",
      "46/250.0 loss: 0.34872985076397023 \n",
      "47/250.0 loss: 0.34697990243633586 \n",
      "48/250.0 loss: 0.3421373308009031 \n",
      "49/250.0 loss: 0.34430201902985574 \n",
      "50/250.0 loss: 0.3414088974104208 \n",
      "51/250.0 loss: 0.3413792225317313 \n",
      "52/250.0 loss: 0.3467485012029702 \n",
      "53/250.0 loss: 0.3425235697240741 \n",
      "54/250.0 loss: 0.338198757307096 \n",
      "55/250.0 loss: 0.334189879574946 \n",
      "56/250.0 loss: 0.3420435237257104 \n",
      "57/250.0 loss: 0.33857993627416677 \n",
      "58/250.0 loss: 0.3358227989431155 \n",
      "59/250.0 loss: 0.34019633680582045 \n",
      "60/250.0 loss: 0.34770696622426395 \n",
      "61/250.0 loss: 0.3576231661342805 \n",
      "62/250.0 loss: 0.3536578518530679 \n",
      "63/250.0 loss: 0.3503401700872928 \n",
      "64/250.0 loss: 0.34657861796709205 \n",
      "65/250.0 loss: 0.34500007624879026 \n",
      "66/250.0 loss: 0.34425228811911684 \n",
      "67/250.0 loss: 0.3463272458928473 \n",
      "68/250.0 loss: 0.34274496462034143 \n",
      "69/250.0 loss: 0.36036634360040937 \n",
      "70/250.0 loss: 0.35661423185341795 \n",
      "71/250.0 loss: 0.35314039430684513 \n",
      "72/250.0 loss: 0.3648951531273045 \n",
      "73/250.0 loss: 0.36149258891472946 \n",
      "74/250.0 loss: 0.3585457354784012 \n",
      "75/250.0 loss: 0.35498427854556786 \n",
      "76/250.0 loss: 0.3513911323887961 \n",
      "77/250.0 loss: 0.3507346912072255 \n",
      "78/250.0 loss: 0.34813232455827015 \n",
      "79/250.0 loss: 0.3470606068149209 \n",
      "80/250.0 loss: 0.3475755050226494 \n",
      "81/250.0 loss: 0.3463955886843728 \n",
      "82/250.0 loss: 0.3437537592218583 \n",
      "83/250.0 loss: 0.34651794373279526 \n",
      "84/250.0 loss: 0.3455389203394161 \n",
      "85/250.0 loss: 0.34300485686507337 \n",
      "86/250.0 loss: 0.3401159746893521 \n",
      "87/250.0 loss: 0.34263089163736865 \n",
      "88/250.0 loss: 0.3415965096334393 \n",
      "89/250.0 loss: 0.3441002779536777 \n",
      "90/250.0 loss: 0.3449304637673137 \n",
      "91/250.0 loss: 0.3422040715003791 \n",
      "92/250.0 loss: 0.3447434616185004 \n",
      "93/250.0 loss: 0.3420712043471793 \n",
      "94/250.0 loss: 0.345963984022015 \n",
      "95/250.0 loss: 0.3489831775271644 \n",
      "96/250.0 loss: 0.34752529944033966 \n",
      "97/250.0 loss: 0.3499657994478333 \n",
      "98/250.0 loss: 0.3478186990727078 \n",
      "99/250.0 loss: 0.3454128251224756 \n",
      "100/250.0 loss: 0.34321978216124055 \n",
      "101/250.0 loss: 0.34545448451649907 \n",
      "102/250.0 loss: 0.34388174854435966 \n",
      "103/250.0 loss: 0.3416266381167449 \n",
      "104/250.0 loss: 0.3405968484424409 \n",
      "105/250.0 loss: 0.33852490937372426 \n",
      "106/250.0 loss: 0.3452348005827342 \n",
      "107/250.0 loss: 0.3433606797070415 \n",
      "108/250.0 loss: 0.3413717902581626 \n",
      "109/250.0 loss: 0.3391633721915158 \n",
      "110/250.0 loss: 0.33694712931776905 \n",
      "111/250.0 loss: 0.3383776190956788 \n",
      "112/250.0 loss: 0.33795876227385174 \n",
      "113/250.0 loss: 0.33594576563490064 \n",
      "114/250.0 loss: 0.34313632139693134 \n",
      "115/250.0 loss: 0.3424623774942653 \n",
      "116/250.0 loss: 0.34035866159913886 \n",
      "117/250.0 loss: 0.34026427371269563 \n",
      "118/250.0 loss: 0.33827741018363405 \n",
      "119/250.0 loss: 0.3367890205234289 \n",
      "120/250.0 loss: 0.33517606113075227 \n",
      "121/250.0 loss: 0.3339220506490254 \n",
      "122/250.0 loss: 0.33769743532184665 \n",
      "123/250.0 loss: 0.3359253801285259 \n",
      "124/250.0 loss: 0.33869594675302506 \n",
      "125/250.0 loss: 0.3365787159474123 \n",
      "126/250.0 loss: 0.33680721560097115 \n",
      "127/250.0 loss: 0.33506678690901026 \n",
      "128/250.0 loss: 0.3357518467214681 \n",
      "129/250.0 loss: 0.3356602562734714 \n",
      "130/250.0 loss: 0.3338085590996815 \n",
      "131/250.0 loss: 0.33643315512348304 \n",
      "132/250.0 loss: 0.34016007862817077 \n",
      "133/250.0 loss: 0.3409329026159066 \n",
      "134/250.0 loss: 0.3451255873949439 \n",
      "135/250.0 loss: 0.344022698536077 \n",
      "136/250.0 loss: 0.3454985455983747 \n",
      "137/250.0 loss: 0.3466925191814485 \n",
      "138/250.0 loss: 0.348296951314957 \n",
      "139/250.0 loss: 0.35001845684434685 \n",
      "140/250.0 loss: 0.3488152543386669 \n",
      "141/250.0 loss: 0.34725779803915763 \n",
      "142/250.0 loss: 0.34537371393892313 \n",
      "143/250.0 loss: 0.34400117981971967 \n",
      "144/250.0 loss: 0.3422350326488758 \n",
      "145/250.0 loss: 0.34051577512123815 \n",
      "146/250.0 loss: 0.33883632628285154 \n",
      "147/250.0 loss: 0.33933977440402313 \n",
      "148/250.0 loss: 0.34019086004903654 \n",
      "149/250.0 loss: 0.34552267372608186 \n",
      "150/250.0 loss: 0.3460636634305613 \n",
      "151/250.0 loss: 0.34425597294772925 \n",
      "152/250.0 loss: 0.3426212468275837 \n",
      "153/250.0 loss: 0.34122856788627515 \n",
      "154/250.0 loss: 0.3399042996187364 \n",
      "155/250.0 loss: 0.3385725464098729 \n",
      "156/250.0 loss: 0.33689995912040116 \n",
      "157/250.0 loss: 0.3351573580522326 \n",
      "158/250.0 loss: 0.33386001211104904 \n",
      "159/250.0 loss: 0.3325489574577659 \n",
      "160/250.0 loss: 0.3324343015595993 \n",
      "161/250.0 loss: 0.33132291543814874 \n",
      "162/250.0 loss: 0.33110667760569623 \n",
      "163/250.0 loss: 0.33179861257170756 \n",
      "164/250.0 loss: 0.3338303669384032 \n",
      "165/250.0 loss: 0.3326736133410988 \n",
      "166/250.0 loss: 0.33114797104440047 \n",
      "167/250.0 loss: 0.3298782855272293 \n",
      "168/250.0 loss: 0.3339821523463232 \n",
      "169/250.0 loss: 0.33302888484562143 \n",
      "170/250.0 loss: 0.33549496339775664 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/250.0 loss: 0.33398375339632813 \n",
      "172/250.0 loss: 0.3348943983027012 \n",
      "173/250.0 loss: 0.3363638262467823 \n",
      "174/250.0 loss: 0.335765489254679 \n",
      "175/250.0 loss: 0.33710010967810045 \n",
      "176/250.0 loss: 0.3363280056391732 \n",
      "177/250.0 loss: 0.33487767554568443 \n",
      "178/250.0 loss: 0.33380285600543685 \n",
      "179/250.0 loss: 0.33232608830763233 \n",
      "180/250.0 loss: 0.3312934771674114 \n",
      "181/250.0 loss: 0.32987504579372456 \n",
      "182/250.0 loss: 0.328988796822686 \n",
      "183/250.0 loss: 0.3285315630795515 \n",
      "184/250.0 loss: 0.32867983978342363 \n",
      "185/250.0 loss: 0.3292953318725991 \n",
      "186/250.0 loss: 0.3307247293744495 \n",
      "187/250.0 loss: 0.3295794382215814 \n",
      "188/250.0 loss: 0.32859430418758795 \n",
      "189/250.0 loss: 0.3277135041199232 \n",
      "190/250.0 loss: 0.3268708809820145 \n",
      "191/250.0 loss: 0.3269809697133799 \n",
      "192/250.0 loss: 0.32614469165320226 \n",
      "193/250.0 loss: 0.3262479358266309 \n",
      "194/250.0 loss: 0.32604522896118654 \n",
      "195/250.0 loss: 0.3247729363490124 \n",
      "196/250.0 loss: 0.32355110907010015 \n",
      "197/250.0 loss: 0.3255759081003642 \n",
      "198/250.0 loss: 0.3242455305877942 \n",
      "199/250.0 loss: 0.32333632377907634 \n",
      "200/250.0 loss: 0.3251494593102837 \n",
      "201/250.0 loss: 0.328012749091676 \n",
      "202/250.0 loss: 0.3266531663208172 \n",
      "203/250.0 loss: 0.32703317508247554 \n",
      "204/250.0 loss: 0.3261051759123802 \n",
      "205/250.0 loss: 0.324983606530914 \n",
      "206/250.0 loss: 0.32443820735538637 \n",
      "207/250.0 loss: 0.3232742705239126 \n",
      "208/250.0 loss: 0.32424102156783974 \n",
      "209/250.0 loss: 0.3255258537119343 \n",
      "210/250.0 loss: 0.3248325801361794 \n",
      "211/250.0 loss: 0.32360222137902145 \n",
      "212/250.0 loss: 0.32246795391112987 \n",
      "213/250.0 loss: 0.32136604757370235 \n",
      "214/250.0 loss: 0.3232617481849914 \n",
      "215/250.0 loss: 0.3260508680164262 \n",
      "216/250.0 loss: 0.3251347134945579 \n",
      "217/250.0 loss: 0.3241063194058904 \n",
      "218/250.0 loss: 0.32309549131758136 \n",
      "219/250.0 loss: 0.32297293432056906 \n",
      "220/250.0 loss: 0.3222114891150958 \n",
      "221/250.0 loss: 0.32580874816657185 \n",
      "222/250.0 loss: 0.324593543972937 \n",
      "223/250.0 loss: 0.3236436817595469 \n",
      "224/250.0 loss: 0.3225850963923666 \n",
      "225/250.0 loss: 0.3271672810403119 \n",
      "226/250.0 loss: 0.3260725779614785 \n",
      "227/250.0 loss: 0.32548978960697067 \n",
      "228/250.0 loss: 0.32695323576963625 \n",
      "229/250.0 loss: 0.3257681804668644 \n",
      "230/250.0 loss: 0.3248096500156504 \n",
      "231/250.0 loss: 0.32371104234058795 \n",
      "232/250.0 loss: 0.32286931767803917 \n",
      "233/250.0 loss: 0.3230117528388898 \n",
      "234/250.0 loss: 0.32650239716501944 \n",
      "235/250.0 loss: 0.3258489445009727 \n",
      "236/250.0 loss: 0.32472791606049495 \n",
      "237/250.0 loss: 0.3238103612474784 \n",
      "238/250.0 loss: 0.3227741294170274 \n",
      "239/250.0 loss: 0.32197251988885306 \n",
      "240/250.0 loss: 0.32096432889831017 \n",
      "241/250.0 loss: 0.3201036980791279 \n",
      "242/250.0 loss: 0.3191255433906498 \n",
      "243/250.0 loss: 0.318297388352698 \n",
      "244/250.0 loss: 0.3198890907271784 \n",
      "245/250.0 loss: 0.31882961341217764 \n",
      "246/250.0 loss: 0.31806770279279606 \n",
      "247/250.0 loss: 0.31821566398045226 \n",
      "248/250.0 loss: 0.3173108455915767 \n",
      "249/250.0 loss: 0.3198731337338686 \n",
      "Epoch:  5\n",
      "0/250.0 loss: 0.08236344903707504 \n",
      "1/250.0 loss: 0.38030407950282097 \n",
      "2/250.0 loss: 0.27825385332107544 \n",
      "3/250.0 loss: 0.34801942110061646 \n",
      "4/250.0 loss: 0.33825643062591554 \n",
      "5/250.0 loss: 0.306040957570076 \n",
      "6/250.0 loss: 0.2783908588545663 \n",
      "7/250.0 loss: 0.2837976813316345 \n",
      "8/250.0 loss: 0.2646290245983336 \n",
      "9/250.0 loss: 0.3095960721373558 \n",
      "10/250.0 loss: 0.31864566830071533 \n",
      "11/250.0 loss: 0.30534842734535533 \n",
      "12/250.0 loss: 0.2922969620961409 \n",
      "13/250.0 loss: 0.2752244089330946 \n",
      "14/250.0 loss: 0.3023154616355896 \n",
      "15/250.0 loss: 0.3219773955643177 \n",
      "16/250.0 loss: 0.3082713379579432 \n",
      "17/250.0 loss: 0.296420194208622 \n",
      "18/250.0 loss: 0.28478996063533585 \n",
      "19/250.0 loss: 0.2844549998641014 \n",
      "20/250.0 loss: 0.27425489858502433 \n",
      "21/250.0 loss: 0.2661265151744539 \n",
      "22/250.0 loss: 0.26154008021821146 \n",
      "23/250.0 loss: 0.25477933293829363 \n",
      "24/250.0 loss: 0.2575966289639473 \n",
      "25/250.0 loss: 0.24980865161006266 \n",
      "26/250.0 loss: 0.24342739444088052 \n",
      "27/250.0 loss: 0.23696084639855794 \n",
      "28/250.0 loss: 0.23053027355465397 \n",
      "29/250.0 loss: 0.23575521036982536 \n",
      "30/250.0 loss: 0.2519122547199649 \n",
      "31/250.0 loss: 0.2492048896383494 \n",
      "32/250.0 loss: 0.24341784349896692 \n",
      "33/250.0 loss: 0.23890019756029635 \n",
      "34/250.0 loss: 0.23379500274147305 \n",
      "35/250.0 loss: 0.25838306235770386 \n",
      "36/250.0 loss: 0.2534450866483353 \n",
      "37/250.0 loss: 0.24919352758871882 \n",
      "38/250.0 loss: 0.25219076833663845 \n",
      "39/250.0 loss: 0.2605989705771208 \n",
      "40/250.0 loss: 0.2662670390635002 \n",
      "41/250.0 loss: 0.27602951122181757 \n",
      "42/250.0 loss: 0.2712179048116817 \n",
      "43/250.0 loss: 0.27394218878312543 \n",
      "44/250.0 loss: 0.2849464694658915 \n",
      "45/250.0 loss: 0.2907340617283531 \n",
      "46/250.0 loss: 0.28569995334490816 \n",
      "47/250.0 loss: 0.29680954657184583 \n",
      "48/250.0 loss: 0.29494915438853964 \n",
      "49/250.0 loss: 0.2905194476991892 \n",
      "50/250.0 loss: 0.28586578632102294 \n",
      "51/250.0 loss: 0.2915519421490339 \n",
      "52/250.0 loss: 0.29632333342759115 \n",
      "53/250.0 loss: 0.2923785795768102 \n",
      "54/250.0 loss: 0.28786929750984364 \n",
      "55/250.0 loss: 0.2841396476807339 \n",
      "56/250.0 loss: 0.28216825360268877 \n",
      "57/250.0 loss: 0.2807836436249059 \n",
      "58/250.0 loss: 0.2780501447239165 \n",
      "59/250.0 loss: 0.27733443640172484 \n",
      "60/250.0 loss: 0.2833198979496956 \n",
      "61/250.0 loss: 0.2796072195614538 \n",
      "62/250.0 loss: 0.28808972144883777 \n",
      "63/250.0 loss: 0.28543559252284467 \n",
      "64/250.0 loss: 0.2826922200047053 \n",
      "65/250.0 loss: 0.28233006494966423 \n",
      "66/250.0 loss: 0.27950480591450166 \n",
      "67/250.0 loss: 0.2774211283334914 \n",
      "68/250.0 loss: 0.27461894137271936 \n",
      "69/250.0 loss: 0.2732247701713017 \n",
      "70/250.0 loss: 0.2702259081350246 \n",
      "71/250.0 loss: 0.26741630935834515 \n",
      "72/250.0 loss: 0.2650737407272809 \n",
      "73/250.0 loss: 0.26254781365797325 \n",
      "74/250.0 loss: 0.2599899350603421 \n",
      "75/250.0 loss: 0.2615100850204104 \n",
      "76/250.0 loss: 0.26775243055897874 \n",
      "77/250.0 loss: 0.26477456040298325 \n",
      "78/250.0 loss: 0.2760455980425394 \n",
      "79/250.0 loss: 0.2733156600035727 \n",
      "80/250.0 loss: 0.2756218881702717 \n",
      "81/250.0 loss: 0.2732254810994718 \n",
      "82/250.0 loss: 0.27049377372106875 \n",
      "83/250.0 loss: 0.26887321942264125 \n",
      "84/250.0 loss: 0.2687393399722436 \n",
      "85/250.0 loss: 0.2662309999036234 \n",
      "86/250.0 loss: 0.2664961062971203 \n",
      "87/250.0 loss: 0.27087824199010024 \n",
      "88/250.0 loss: 0.271156195341871 \n",
      "89/250.0 loss: 0.27977758993705115 \n",
      "90/250.0 loss: 0.27962685106219826 \n",
      "91/250.0 loss: 0.27842489937725273 \n",
      "92/250.0 loss: 0.2768520978830194 \n",
      "93/250.0 loss: 0.2745174681965975 \n",
      "94/250.0 loss: 0.27767840925800175 \n",
      "95/250.0 loss: 0.275174625722381 \n",
      "96/250.0 loss: 0.2733503548217188 \n",
      "97/250.0 loss: 0.2714227840532454 \n",
      "98/250.0 loss: 0.26936123403485374 \n",
      "99/250.0 loss: 0.26712042063474656 \n",
      "100/250.0 loss: 0.2661272709322448 \n",
      "101/250.0 loss: 0.2640831245511186 \n",
      "102/250.0 loss: 0.26214381842647944 \n",
      "103/250.0 loss: 0.25995461950795007 \n",
      "104/250.0 loss: 0.26714214561950594 \n",
      "105/250.0 loss: 0.2653392826751718 \n",
      "106/250.0 loss: 0.2637142268157451 \n",
      "107/250.0 loss: 0.261695003343953 \n",
      "108/250.0 loss: 0.264838852740209 \n",
      "109/250.0 loss: 0.26712748272852466 \n",
      "110/250.0 loss: 0.26654930533589544 \n",
      "111/250.0 loss: 0.2649654144687312 \n",
      "112/250.0 loss: 0.2631927818983002 \n",
      "113/250.0 loss: 0.26716230685512227 \n",
      "114/250.0 loss: 0.2702095374464989 \n",
      "115/250.0 loss: 0.27363143508033505 \n",
      "116/250.0 loss: 0.27185275418381405 \n",
      "117/250.0 loss: 0.2700779146625329 \n",
      "118/250.0 loss: 0.2683424398991741 \n",
      "119/250.0 loss: 0.27206719868505996 \n",
      "120/250.0 loss: 0.27706797953602696 \n",
      "121/250.0 loss: 0.28029308996361785 \n",
      "122/250.0 loss: 0.28065357511726824 \n",
      "123/250.0 loss: 0.2788530322873304 \n",
      "124/250.0 loss: 0.2826250709593296 \n",
      "125/250.0 loss: 0.28075293578680544 \n",
      "126/250.0 loss: 0.27906116441361545 \n",
      "127/250.0 loss: 0.277368675917387 \n",
      "128/250.0 loss: 0.27602977143932683 \n",
      "129/250.0 loss: 0.2744693634028618 \n",
      "130/250.0 loss: 0.27329879515726146 \n",
      "131/250.0 loss: 0.27169403941793874 \n",
      "132/250.0 loss: 0.27074040125187177 \n",
      "133/250.0 loss: 0.2691598444787869 \n",
      "134/250.0 loss: 0.2688900975441491 \n",
      "135/250.0 loss: 0.2711792247753371 \n",
      "136/250.0 loss: 0.26949186927645746 \n",
      "137/250.0 loss: 0.2720593046666919 \n",
      "138/250.0 loss: 0.27127363366617574 \n",
      "139/250.0 loss: 0.27060626702649254 \n",
      "140/250.0 loss: 0.2703103541482425 \n",
      "141/250.0 loss: 0.27107375944164436 \n",
      "142/250.0 loss: 0.2711317016111387 \n",
      "143/250.0 loss: 0.27411253977980876 \n",
      "144/250.0 loss: 0.2755189490729365 \n",
      "145/250.0 loss: 0.2740370663367722 \n",
      "146/250.0 loss: 0.27788201273501323 \n",
      "147/250.0 loss: 0.28070511281288957 \n",
      "148/250.0 loss: 0.2791903992527283 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/250.0 loss: 0.2778594579299291 \n",
      "150/250.0 loss: 0.2764188405989811 \n",
      "151/250.0 loss: 0.27578423226154164 \n",
      "152/250.0 loss: 0.27709338531579847 \n",
      "153/250.0 loss: 0.2756300452467683 \n",
      "154/250.0 loss: 0.2784763789946033 \n",
      "155/250.0 loss: 0.28111778390713227 \n",
      "156/250.0 loss: 0.27966238487108497 \n",
      "157/250.0 loss: 0.27821058808248256 \n",
      "158/250.0 loss: 0.2767484505315247 \n",
      "159/250.0 loss: 0.2779676512349397 \n",
      "160/250.0 loss: 0.27661622584051226 \n",
      "161/250.0 loss: 0.28171567319903845 \n",
      "162/250.0 loss: 0.2831914546939493 \n",
      "163/250.0 loss: 0.2817991009300075 \n",
      "164/250.0 loss: 0.280374255934448 \n",
      "165/250.0 loss: 0.2817937062761511 \n",
      "166/250.0 loss: 0.28047936710858057 \n",
      "167/250.0 loss: 0.2792643685381682 \n",
      "168/250.0 loss: 0.2819688063357356 \n",
      "169/250.0 loss: 0.2806292149512207 \n",
      "170/250.0 loss: 0.28250250982785086 \n",
      "171/250.0 loss: 0.2812374587235756 \n",
      "172/250.0 loss: 0.2798831544687293 \n",
      "173/250.0 loss: 0.2786172437428058 \n",
      "174/250.0 loss: 0.27740669344152724 \n",
      "175/250.0 loss: 0.27917600550096144 \n",
      "176/250.0 loss: 0.27873598788417664 \n",
      "177/250.0 loss: 0.2807200618004531 \n",
      "178/250.0 loss: 0.2794867030360512 \n",
      "179/250.0 loss: 0.28169679035329154 \n",
      "180/250.0 loss: 0.2837508833498915 \n",
      "181/250.0 loss: 0.2823546294143403 \n",
      "182/250.0 loss: 0.2811475021082684 \n",
      "183/250.0 loss: 0.28010623273439705 \n",
      "184/250.0 loss: 0.2837539313914808 \n",
      "185/250.0 loss: 0.28252789094763736 \n",
      "186/250.0 loss: 0.2816118147463244 \n",
      "187/250.0 loss: 0.2812657629417137 \n",
      "188/250.0 loss: 0.2801306263183948 \n",
      "189/250.0 loss: 0.279018400776151 \n",
      "190/250.0 loss: 0.27858465021544415 \n",
      "191/250.0 loss: 0.27740481979950954 \n",
      "192/250.0 loss: 0.2763360883036889 \n",
      "193/250.0 loss: 0.2817711525107014 \n",
      "194/250.0 loss: 0.2805905055445738 \n",
      "195/250.0 loss: 0.27938200475419966 \n",
      "196/250.0 loss: 0.27841946198061335 \n",
      "197/250.0 loss: 0.277295421460888 \n",
      "198/250.0 loss: 0.27663859536861957 \n",
      "199/250.0 loss: 0.2755226881150156 \n",
      "200/250.0 loss: 0.27500749391443396 \n",
      "201/250.0 loss: 0.2773309983036453 \n",
      "202/250.0 loss: 0.2762164132476881 \n",
      "203/250.0 loss: 0.2750215650463075 \n",
      "204/250.0 loss: 0.27392536613454177 \n",
      "205/250.0 loss: 0.27286023962157735 \n",
      "206/250.0 loss: 0.2718376716427901 \n",
      "207/250.0 loss: 0.2742480040044309 \n",
      "208/250.0 loss: 0.27324408040555753 \n",
      "209/250.0 loss: 0.27239854879499903 \n",
      "210/250.0 loss: 0.2712890477630341 \n",
      "211/250.0 loss: 0.2701899238932386 \n",
      "212/250.0 loss: 0.27107767233441415 \n",
      "213/250.0 loss: 0.27490918461024483 \n",
      "214/250.0 loss: 0.27384412393493707 \n",
      "215/250.0 loss: 0.2763814834481174 \n",
      "216/250.0 loss: 0.2772952037421377 \n",
      "217/250.0 loss: 0.2784604574510947 \n",
      "218/250.0 loss: 0.2775103495494552 \n",
      "219/250.0 loss: 0.2766804771751843 \n",
      "220/250.0 loss: 0.27574465792137304 \n",
      "221/250.0 loss: 0.27476475158756647 \n",
      "222/250.0 loss: 0.2737224287031878 \n",
      "223/250.0 loss: 0.2770470309520273 \n",
      "224/250.0 loss: 0.27939354822867446 \n",
      "225/250.0 loss: 0.2784528217270179 \n",
      "226/250.0 loss: 0.2774056790207845 \n",
      "227/250.0 loss: 0.2764320338814797 \n",
      "228/250.0 loss: 0.277013926713943 \n",
      "229/250.0 loss: 0.2761061792707314 \n",
      "230/250.0 loss: 0.27508490810823905 \n",
      "231/250.0 loss: 0.2776265868612019 \n",
      "232/250.0 loss: 0.27658455822888617 \n",
      "233/250.0 loss: 0.2755741970613599 \n",
      "234/250.0 loss: 0.27460315714966743 \n",
      "235/250.0 loss: 0.27364895754036006 \n",
      "236/250.0 loss: 0.27270105527517413 \n",
      "237/250.0 loss: 0.2720712571318786 \n",
      "238/250.0 loss: 0.27110426678910665 \n",
      "239/250.0 loss: 0.27029571786988527 \n",
      "240/250.0 loss: 0.26953899207296955 \n",
      "241/250.0 loss: 0.2719428289512342 \n",
      "242/250.0 loss: 0.27095264033126976 \n",
      "243/250.0 loss: 0.2701141432339906 \n",
      "244/250.0 loss: 0.2698224217839995 \n",
      "245/250.0 loss: 0.26908119761089727 \n",
      "246/250.0 loss: 0.2681968255036394 \n",
      "247/250.0 loss: 0.2672382338167799 \n",
      "248/250.0 loss: 0.2664301430799037 \n",
      "249/250.0 loss: 0.2657260457351804 \n",
      "Epoch:  6\n",
      "0/250.0 loss: 0.04412726312875748 \n",
      "1/250.0 loss: 0.04253182001411915 \n",
      "2/250.0 loss: 0.06797856216629346 \n",
      "3/250.0 loss: 0.06083900388330221 \n",
      "4/250.0 loss: 0.21988626047968865 \n",
      "5/250.0 loss: 0.19121338985860348 \n",
      "6/250.0 loss: 0.17453158700040408 \n",
      "7/250.0 loss: 0.15764735033735633 \n",
      "8/250.0 loss: 0.15426786699228817 \n",
      "9/250.0 loss: 0.14461819976568221 \n",
      "10/250.0 loss: 0.1651156178929589 \n",
      "11/250.0 loss: 0.15350030238429704 \n",
      "12/250.0 loss: 0.14357851431346857 \n",
      "13/250.0 loss: 0.13652135178978955 \n",
      "14/250.0 loss: 0.1299766954034567 \n",
      "15/250.0 loss: 0.1469037007773295 \n",
      "16/250.0 loss: 0.14058724319671884 \n",
      "17/250.0 loss: 0.18710109684616327 \n",
      "18/250.0 loss: 0.17961569061796917 \n",
      "19/250.0 loss: 0.1721959318034351 \n",
      "20/250.0 loss: 0.16669579302625998 \n",
      "21/250.0 loss: 0.18550188932567835 \n",
      "22/250.0 loss: 0.1796180362124806 \n",
      "23/250.0 loss: 0.17374762543477118 \n",
      "24/250.0 loss: 0.1688682273775339 \n",
      "25/250.0 loss: 0.17929330389373577 \n",
      "26/250.0 loss: 0.17349640769815003 \n",
      "27/250.0 loss: 0.17942187210012758 \n",
      "28/250.0 loss: 0.174450954550813 \n",
      "29/250.0 loss: 0.1694923212751746 \n",
      "30/250.0 loss: 0.16515091908795218 \n",
      "31/250.0 loss: 0.1609031299012713 \n",
      "32/250.0 loss: 0.17906829641398156 \n",
      "33/250.0 loss: 0.17603024243212798 \n",
      "34/250.0 loss: 0.179718101184283 \n",
      "35/250.0 loss: 0.17606660946168834 \n",
      "36/250.0 loss: 0.17379522540077968 \n",
      "37/250.0 loss: 0.16976547873529949 \n",
      "38/250.0 loss: 0.17887780151497096 \n",
      "39/250.0 loss: 0.17530956356786193 \n",
      "40/250.0 loss: 0.17153628511218036 \n",
      "41/250.0 loss: 0.18459139635697716 \n",
      "42/250.0 loss: 0.18134161236501017 \n",
      "43/250.0 loss: 0.19675351573492994 \n",
      "44/250.0 loss: 0.20923918911980258 \n",
      "45/250.0 loss: 0.20539459493011236 \n",
      "46/250.0 loss: 0.20173867684888078 \n",
      "47/250.0 loss: 0.19846454577054828 \n",
      "48/250.0 loss: 0.19527548859466096 \n",
      "49/250.0 loss: 0.19180552016943694 \n",
      "50/250.0 loss: 0.18861759574536016 \n",
      "51/250.0 loss: 0.19680111271400863 \n",
      "52/250.0 loss: 0.20959719442674574 \n",
      "53/250.0 loss: 0.2065169310749129 \n",
      "54/250.0 loss: 0.20371184372766452 \n",
      "55/250.0 loss: 0.21367456042207778 \n",
      "56/250.0 loss: 0.21053259829549412 \n",
      "57/250.0 loss: 0.2075964962292848 \n",
      "58/250.0 loss: 0.20478540801016962 \n",
      "59/250.0 loss: 0.2019043268325428 \n",
      "60/250.0 loss: 0.19952843390160896 \n",
      "61/250.0 loss: 0.19679655918791408 \n",
      "62/250.0 loss: 0.19475211809197115 \n",
      "63/250.0 loss: 0.19221570322406478 \n",
      "64/250.0 loss: 0.18989843843648066 \n",
      "65/250.0 loss: 0.1889387656172568 \n",
      "66/250.0 loss: 0.18683641584617877 \n",
      "67/250.0 loss: 0.1844899441279909 \n",
      "68/250.0 loss: 0.18301791519574498 \n",
      "69/250.0 loss: 0.18066494701696292 \n",
      "70/250.0 loss: 0.1785542188658261 \n",
      "71/250.0 loss: 0.19089575900903177 \n",
      "72/250.0 loss: 0.18885963014645935 \n",
      "73/250.0 loss: 0.19294816177539728 \n",
      "74/250.0 loss: 0.19072877044479053 \n",
      "75/250.0 loss: 0.18861728482634613 \n",
      "76/250.0 loss: 0.1864570240647375 \n",
      "77/250.0 loss: 0.18752067142094558 \n",
      "78/250.0 loss: 0.1867368621869555 \n",
      "79/250.0 loss: 0.19036375291179866 \n",
      "80/250.0 loss: 0.18825180945848977 \n",
      "81/250.0 loss: 0.1930946779278357 \n",
      "82/250.0 loss: 0.19104042085987258 \n",
      "83/250.0 loss: 0.18897822554711075 \n",
      "84/250.0 loss: 0.187072201226564 \n",
      "85/250.0 loss: 0.1851459124970228 \n",
      "86/250.0 loss: 0.18334740259010215 \n",
      "87/250.0 loss: 0.18165003368631005 \n",
      "88/250.0 loss: 0.1799033311394493 \n",
      "89/250.0 loss: 0.19478804075883493 \n",
      "90/250.0 loss: 0.19363740811144914 \n",
      "91/250.0 loss: 0.19201939524677786 \n",
      "92/250.0 loss: 0.19046323021413178 \n",
      "93/250.0 loss: 0.19739089054154588 \n",
      "94/250.0 loss: 0.19563763812184334 \n",
      "95/250.0 loss: 0.19396504329051822 \n",
      "96/250.0 loss: 0.19273305711211616 \n",
      "97/250.0 loss: 0.19829748696362487 \n",
      "98/250.0 loss: 0.19666180033424888 \n",
      "99/250.0 loss: 0.19507978912442922 \n",
      "100/250.0 loss: 0.1933973151083925 \n",
      "101/250.0 loss: 0.19180401683072834 \n",
      "102/250.0 loss: 0.19027474933095928 \n",
      "103/250.0 loss: 0.1937189724547072 \n",
      "104/250.0 loss: 0.19208510853350164 \n",
      "105/250.0 loss: 0.1906568377261173 \n",
      "106/250.0 loss: 0.20436579519969839 \n",
      "107/250.0 loss: 0.20320032752567418 \n",
      "108/250.0 loss: 0.2017528826046154 \n",
      "109/250.0 loss: 0.20215375340459021 \n",
      "110/250.0 loss: 0.20102118980978523 \n",
      "111/250.0 loss: 0.1995027110679075 \n",
      "112/250.0 loss: 0.19807700471606401 \n",
      "113/250.0 loss: 0.1965985414980535 \n",
      "114/250.0 loss: 0.19505309266564638 \n",
      "115/250.0 loss: 0.19356777613725643 \n",
      "116/250.0 loss: 0.19217029844339079 \n",
      "117/250.0 loss: 0.19068635734966247 \n",
      "118/250.0 loss: 0.18932552003309505 \n",
      "119/250.0 loss: 0.18807879233111938 \n",
      "120/250.0 loss: 0.18677059650297992 \n",
      "121/250.0 loss: 0.18542780776004322 \n",
      "122/250.0 loss: 0.1918163022132424 \n",
      "123/250.0 loss: 0.19050426775168988 \n",
      "124/250.0 loss: 0.19520174545049668 \n",
      "125/250.0 loss: 0.19390695518444453 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/250.0 loss: 0.1975181594373673 \n",
      "127/250.0 loss: 0.19621287191694137 \n",
      "128/250.0 loss: 0.19500945668754188 \n",
      "129/250.0 loss: 0.19665306312246963 \n",
      "130/250.0 loss: 0.19605748132155595 \n",
      "131/250.0 loss: 0.19934472276575185 \n",
      "132/250.0 loss: 0.19872548474573104 \n",
      "133/250.0 loss: 0.20212131420345003 \n",
      "134/250.0 loss: 0.20135447784430452 \n",
      "135/250.0 loss: 0.20023297643124618 \n",
      "136/250.0 loss: 0.19929145095720344 \n",
      "137/250.0 loss: 0.1984501311933433 \n",
      "138/250.0 loss: 0.19717545647897738 \n",
      "139/250.0 loss: 0.1965162896817284 \n",
      "140/250.0 loss: 0.1953584531071127 \n",
      "141/250.0 loss: 0.19614780939538295 \n",
      "142/250.0 loss: 0.19533670786116925 \n",
      "143/250.0 loss: 0.19417107556687874 \n",
      "144/250.0 loss: 0.19295639304508422 \n",
      "145/250.0 loss: 0.19179633947421018 \n",
      "146/250.0 loss: 0.19443187212609514 \n",
      "147/250.0 loss: 0.19337158299026055 \n",
      "148/250.0 loss: 0.19289061803395716 \n",
      "149/250.0 loss: 0.1917723848298192 \n",
      "150/250.0 loss: 0.190794910557124 \n",
      "151/250.0 loss: 0.18978933503508175 \n",
      "152/250.0 loss: 0.18863129041772458 \n",
      "153/250.0 loss: 0.18754468332710011 \n",
      "154/250.0 loss: 0.18678257270085236 \n",
      "155/250.0 loss: 0.18573263883948898 \n",
      "156/250.0 loss: 0.18567747204522989 \n",
      "157/250.0 loss: 0.18469684285147092 \n",
      "158/250.0 loss: 0.1847674109732861 \n",
      "159/250.0 loss: 0.18498723839293235 \n",
      "160/250.0 loss: 0.18404016645497973 \n",
      "161/250.0 loss: 0.18308232983942202 \n",
      "162/250.0 loss: 0.18735785884611264 \n",
      "163/250.0 loss: 0.18633073808557196 \n",
      "164/250.0 loss: 0.1853322815883792 \n",
      "165/250.0 loss: 0.19524555026654558 \n",
      "166/250.0 loss: 0.2009490506366609 \n",
      "167/250.0 loss: 0.20315642009067925 \n",
      "168/250.0 loss: 0.2064621091583336 \n",
      "169/250.0 loss: 0.20538651171304723 \n",
      "170/250.0 loss: 0.20434298184945396 \n",
      "171/250.0 loss: 0.20363247686412272 \n",
      "172/250.0 loss: 0.20270844408434768 \n",
      "173/250.0 loss: 0.2018898737231462 \n",
      "174/250.0 loss: 0.20087115783244372 \n",
      "175/250.0 loss: 0.1999167792276818 \n",
      "176/250.0 loss: 0.19894893593181157 \n",
      "177/250.0 loss: 0.19797754767507816 \n",
      "178/250.0 loss: 0.19699936738373014 \n",
      "179/250.0 loss: 0.1990511804767367 \n",
      "180/250.0 loss: 0.19951931053247735 \n",
      "181/250.0 loss: 0.20349693756325396 \n",
      "182/250.0 loss: 0.20251247398249927 \n",
      "183/250.0 loss: 0.20165349099197952 \n",
      "184/250.0 loss: 0.20374552633210613 \n",
      "185/250.0 loss: 0.20345623472264857 \n",
      "186/250.0 loss: 0.20259643271864258 \n",
      "187/250.0 loss: 0.20165515361611355 \n",
      "188/250.0 loss: 0.2006666918566066 \n",
      "189/250.0 loss: 0.19979187570218193 \n",
      "190/250.0 loss: 0.19901215072955292 \n",
      "191/250.0 loss: 0.1981259892199887 \n",
      "192/250.0 loss: 0.19727503651646908 \n",
      "193/250.0 loss: 0.19638983317238 \n",
      "194/250.0 loss: 0.19730521084692998 \n",
      "195/250.0 loss: 0.19645104345827535 \n",
      "196/250.0 loss: 0.19629292985717506 \n",
      "197/250.0 loss: 0.19935456612100355 \n",
      "198/250.0 loss: 0.19842813086307529 \n",
      "199/250.0 loss: 0.198542916784063 \n",
      "200/250.0 loss: 0.19767145340826678 \n",
      "201/250.0 loss: 0.1968144723721365 \n",
      "202/250.0 loss: 0.1961935423681595 \n",
      "203/250.0 loss: 0.19531542271850447 \n",
      "204/250.0 loss: 0.19451318589470734 \n",
      "205/250.0 loss: 0.19370315226644857 \n",
      "206/250.0 loss: 0.19290807305110824 \n",
      "207/250.0 loss: 0.19215353508479893 \n",
      "208/250.0 loss: 0.19521355131667767 \n",
      "209/250.0 loss: 0.19441000356205873 \n",
      "210/250.0 loss: 0.19385319226090378 \n",
      "211/250.0 loss: 0.193069730621745 \n",
      "212/250.0 loss: 0.19228077473494928 \n",
      "213/250.0 loss: 0.19146085352088524 \n",
      "214/250.0 loss: 0.19479364055360474 \n",
      "215/250.0 loss: 0.19398387534440392 \n",
      "216/250.0 loss: 0.19452986271390993 \n",
      "217/250.0 loss: 0.19376829286199918 \n",
      "218/250.0 loss: 0.19349496399124735 \n",
      "219/250.0 loss: 0.19285538625818763 \n",
      "220/250.0 loss: 0.19210397447901884 \n",
      "221/250.0 loss: 0.19135888883223137 \n",
      "222/250.0 loss: 0.1937592859075075 \n",
      "223/250.0 loss: 0.19299669065679023 \n",
      "224/250.0 loss: 0.1939326893703805 \n",
      "225/250.0 loss: 0.19315897422817957 \n",
      "226/250.0 loss: 0.19239918949976892 \n",
      "227/250.0 loss: 0.19490591701316207 \n",
      "228/250.0 loss: 0.19411876120128746 \n",
      "229/250.0 loss: 0.1936456293758491 \n",
      "230/250.0 loss: 0.19286281466322802 \n",
      "231/250.0 loss: 0.19213974699859732 \n",
      "232/250.0 loss: 0.19145571772216982 \n",
      "233/250.0 loss: 0.19075154727245244 \n",
      "234/250.0 loss: 0.1900372483391077 \n",
      "235/250.0 loss: 0.19181142631381498 \n",
      "236/250.0 loss: 0.19113489555564359 \n",
      "237/250.0 loss: 0.19039100664080816 \n",
      "238/250.0 loss: 0.18963951241832896 \n",
      "239/250.0 loss: 0.188927801962321 \n",
      "240/250.0 loss: 0.18825185955869211 \n",
      "241/250.0 loss: 0.1877368061903341 \n",
      "242/250.0 loss: 0.18704954537835142 \n",
      "243/250.0 loss: 0.19023308597627234 \n",
      "244/250.0 loss: 0.1938141389768951 \n",
      "245/250.0 loss: 0.19310701845925513 \n",
      "246/250.0 loss: 0.19596232777001404 \n",
      "247/250.0 loss: 0.19524288970616557 \n",
      "248/250.0 loss: 0.19591482324772572 \n",
      "249/250.0 loss: 0.19711339032649994 \n",
      "Epoch:  7\n",
      "0/250.0 loss: 0.016428746283054352 \n",
      "1/250.0 loss: 0.022889044135808945 \n",
      "2/250.0 loss: 0.029514530052741367 \n",
      "3/250.0 loss: 0.029032036662101746 \n",
      "4/250.0 loss: 0.02943686433136463 \n",
      "5/250.0 loss: 0.02779249909023444 \n",
      "6/250.0 loss: 0.026311500264065608 \n",
      "7/250.0 loss: 0.025241541676223278 \n",
      "8/250.0 loss: 0.03881584852933884 \n",
      "9/250.0 loss: 0.13651987835764884 \n",
      "10/250.0 loss: 0.13573218407956036 \n",
      "11/250.0 loss: 0.12643312073002258 \n",
      "12/250.0 loss: 0.1182889540034991 \n",
      "13/250.0 loss: 0.1456253243876355 \n",
      "14/250.0 loss: 0.1611291083196799 \n",
      "15/250.0 loss: 0.152644706889987 \n",
      "16/250.0 loss: 0.14468209686524727 \n",
      "17/250.0 loss: 0.1373654863693648 \n",
      "18/250.0 loss: 0.13244591388655336 \n",
      "19/250.0 loss: 0.12675288785248995 \n",
      "20/250.0 loss: 0.12227577379062063 \n",
      "21/250.0 loss: 0.11752471700310707 \n",
      "22/250.0 loss: 0.11349589491020078 \n",
      "23/250.0 loss: 0.13702085443461934 \n",
      "24/250.0 loss: 0.17139533832669257 \n",
      "25/250.0 loss: 0.16553590059853518 \n",
      "26/250.0 loss: 0.16282390375380162 \n",
      "27/250.0 loss: 0.15843594167381525 \n",
      "28/250.0 loss: 0.15997966956989518 \n",
      "29/250.0 loss: 0.15529794904092947 \n",
      "30/250.0 loss: 0.15087447543778726 \n",
      "31/250.0 loss: 0.1466164087469224 \n",
      "32/250.0 loss: 0.14290541277803254 \n",
      "33/250.0 loss: 0.13955667519065387 \n",
      "34/250.0 loss: 0.1361152799267854 \n",
      "35/250.0 loss: 0.1328063879110333 \n",
      "36/250.0 loss: 0.12953942679372188 \n",
      "37/250.0 loss: 0.15333647935308123 \n",
      "38/250.0 loss: 0.15028113344063362 \n",
      "39/250.0 loss: 0.14698381593916565 \n",
      "40/250.0 loss: 0.1543936985885588 \n",
      "41/250.0 loss: 0.15120034365515625 \n",
      "42/250.0 loss: 0.14829214811758246 \n",
      "43/250.0 loss: 0.14526670667427508 \n",
      "44/250.0 loss: 0.14245250013967356 \n",
      "45/250.0 loss: 0.13966803726456736 \n",
      "46/250.0 loss: 0.13699325154277872 \n",
      "47/250.0 loss: 0.17012078621579954 \n",
      "48/250.0 loss: 0.16697831413879685 \n",
      "49/250.0 loss: 0.1642321551963687 \n",
      "50/250.0 loss: 0.1615450347536335 \n",
      "51/250.0 loss: 0.15863707113581207 \n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    \n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        \n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\n",
    "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_df) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHFWhkRYHv5l"
   },
   "outputs": [],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vg_sX9BjooL-",
    "outputId": "2e85f485-c125-4ed6-bb8a-07ed708c54f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bert_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-DmIJqUnkVM8",
    "outputId": "de735d0b-ad04-4d81-ef14-c6394bab92d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.68      0.77        50\n",
      "        True       0.74      0.92      0.82        50\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       100\n",
      "   macro avg       0.82      0.80      0.80       100\n",
      "weighted avg       0.82      0.80      0.80       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIBvoExLpOne"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT to the rescue.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
